{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f358a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 10:19:22.225704: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "import sys \n",
    "from torchinfo import summary\n",
    "from scipy import signal\n",
    "import pyVHR as vhr\n",
    "import pickle\n",
    "from typing import Optional\n",
    " \n",
    "from TorchLossComputer import TorchLossComputer\n",
    "from TorchLossComputerCPU import TorchLossComputerCPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed817b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/datasets/VHR1/PURE\n"
     ]
    }
   ],
   "source": [
    "PATCH_SIZE = 16\n",
    "EMBED_DIM = PATCH_SIZE * PATCH_SIZE * 3\n",
    "NUM_PATCHES = 100\n",
    "IMG_SIZE = PATCH_SIZE * NUM_PATCHES\n",
    "HEADS = 12\n",
    "BLOCKS = 12\n",
    "BATCH = 300\n",
    "LENGTH = 160\n",
    "\n",
    "vhr.plot.VisualizeParams.renderer = 'notebook'  # or 'notebook'\n",
    "\n",
    "dataset_name = 'pure'           \n",
    "video_DIR = '/var/datasets/VHR1/'  \n",
    "BVP_DIR = '/var/datasets/VHR1/'    \n",
    "\n",
    "dataset = vhr.datasets.datasetFactory(dataset_name, videodataDIR=video_DIR, BVPdataDIR=BVP_DIR)\n",
    "allvideo = dataset.videoFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f234d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class CDC_T(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.6):\n",
    "\n",
    "        super(CDC_T, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                              dilation=dilation, groups=groups, bias=bias)\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "\n",
    "        if math.fabs(self.theta - 0.0) < 1e-8:\n",
    "            return out_normal\n",
    "        else:\n",
    "            # pdb.set_trace()\n",
    "            [C_out, C_in, t, kernel_size, kernel_size] = self.conv.weight.shape\n",
    "\n",
    "            # only CD works on temporal kernel size>1\n",
    "            if self.conv.weight.shape[2] > 1:\n",
    "                kernel_diff = self.conv.weight[:, :, 0, :, :].sum(2).sum(2) + self.conv.weight[:, :, 2, :, :].sum(\n",
    "                    2).sum(2)\n",
    "                kernel_diff = kernel_diff[:, :, None, None, None]\n",
    "                out_diff = F.conv3d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,\n",
    "                                    padding=0, dilation=self.conv.dilation, groups=self.conv.groups)\n",
    "                return out_normal - self.theta * out_diff\n",
    "\n",
    "            else:\n",
    "                return out_normal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_last(x, shape):\n",
    "    shape = list(shape)\n",
    "    assert shape.count(-1) <= 1\n",
    "    if -1 in shape:\n",
    "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
    "    return x.view(*x.size()[:-1], *shape)\n",
    "\n",
    "\n",
    "def merge_last(x, n_dims):\n",
    "    s = x.size()\n",
    "    assert n_dims > 1 and n_dims < len(s)\n",
    "    return x.view(*s[:-n_dims], -1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedSelfAttention_TDC_gra_sharp(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout, theta):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_q = nn.Sequential(\n",
    "            CDC_T(dim, dim, 3, stride=1, padding=1, groups=1, bias=False, theta=theta),  \n",
    "            nn.BatchNorm3d(dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.proj_k = nn.Sequential(\n",
    "            CDC_T(dim, dim, 3, stride=1, padding=1, groups=1, bias=False, theta=theta),  \n",
    "            nn.BatchNorm3d(dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.proj_v = nn.Sequential(\n",
    "            nn.Conv3d(dim, dim, 1, stride=1, padding=0, groups=1, bias=False),  \n",
    "            #nn.BatchNorm3d(dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        #self.proj_q = nn.Linear(dim, dim)\n",
    "        #self.proj_k = nn.Linear(dim, dim)\n",
    "        #self.proj_v = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.n_heads = num_heads\n",
    "        self.scores = None # for visualization\n",
    "\n",
    "    def forward(self, x, gra_sharp):    # [B, 4*4*40, 128]\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        \n",
    "        [B, P, C]=x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, P//8, 4, 2)      # [B, dim, 40, 4, 4]\n",
    "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
    "        q = q.flatten(2).transpose(1, 2)  # [B, 4*4*40, dim]\n",
    "        k = k.flatten(2).transpose(1, 2)  # [B, 4*4*40, dim]\n",
    "        v = v.flatten(2).transpose(1, 2)  # [B, 4*4*40, dim]\n",
    "        \n",
    "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])\n",
    "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
    "        scores = q @ k.transpose(-2, -1) / gra_sharp\n",
    "\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        # -merge-> (B, S, D)\n",
    "        h = merge_last(h, 2)\n",
    "        self.scores = scores\n",
    "        return h, scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward_ST(nn.Module):\n",
    "    \"\"\"FeedForward Neural Networks for each position\"\"\"\n",
    "    def __init__(self, dim, ff_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Conv3d(dim, ff_dim, 1, stride=1, padding=0, bias=False),  \n",
    "            nn.BatchNorm3d(ff_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.STConv = nn.Sequential(\n",
    "            nn.Conv3d(ff_dim, ff_dim, 3, stride=1, padding=1, groups=ff_dim, bias=False),  \n",
    "            nn.BatchNorm3d(ff_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Conv3d(ff_dim, dim, 1, stride=1, padding=0, bias=False),  \n",
    "            nn.BatchNorm3d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):    # [B, 4*4*40, 128]\n",
    "        [B, P, C]=x.shape\n",
    "        #x = x.transpose(1, 2).view(B, C, 40, 4, 4)      # [B, dim, 40, 4, 4]\n",
    "        x = x.transpose(1, 2).view(B, C, P//8, 4, 2)      # [B, dim, 40, 4, 4]\n",
    "        x = self.fc1(x)\t\t              # x [B, ff_dim, 40, 4, 4]\n",
    "        x = self.STConv(x)\t\t          # x [B, ff_dim, 40, 4, 4]\n",
    "        x = self.fc2(x)\t\t              # x [B, dim, 40, 4, 4]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, 4*4*40, dim]\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
    "        #return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Block_ST_TDC_gra_sharp(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "    def __init__(self, dim, num_heads, ff_dim, dropout, theta):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedSelfAttention_TDC_gra_sharp(dim, num_heads, dropout, theta)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwff = PositionWiseFeedForward_ST(dim, ff_dim)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, gra_sharp):\n",
    "        Atten, Score = self.attn(self.norm1(x), gra_sharp)\n",
    "        h = self.drop(self.proj(Atten))\n",
    "        x = x + h\n",
    "        h = self.drop(self.pwff(self.norm2(x)))\n",
    "        x = x + h\n",
    "        return x, Score\n",
    "\n",
    "\n",
    "class Transformer_ST_TDC_gra_sharp(nn.Module):\n",
    "    \"\"\"Transformer with Self-Attentive Blocks\"\"\"\n",
    "    def __init__(self, num_layers, dim, num_heads, ff_dim, dropout, theta):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block_ST_TDC_gra_sharp(dim, num_heads, ff_dim, dropout, theta) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, gra_sharp):\n",
    "        for block in self.blocks:\n",
    "            x, Score = block(x, gra_sharp)\n",
    "        return x, Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e515895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def as_tuple(x):\n",
    "    return x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "\n",
    "class CDC_T(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.6):\n",
    "\n",
    "        super(CDC_T, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                              dilation=dilation, groups=groups, bias=bias)\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "\n",
    "        if math.fabs(self.theta - 0.0) < 1e-8:\n",
    "            return out_normal\n",
    "        else:\n",
    "            # pdb.set_trace()\n",
    "            [C_out, C_in, t, kernel_size, kernel_size] = self.conv.weight.shape\n",
    "\n",
    "            # only CD works on temporal kernel size>1\n",
    "            if self.conv.weight.shape[2] > 1:\n",
    "                kernel_diff = self.conv.weight[:, :, 0, :, :].sum(2).sum(2) + self.conv.weight[:, :, 2, :, :].sum(\n",
    "                    2).sum(2)\n",
    "                kernel_diff = kernel_diff[:, :, None, None, None]\n",
    "                out_diff = F.conv3d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride,\n",
    "                                    padding=0, dilation=self.conv.dilation, groups=self.conv.groups)\n",
    "                return out_normal - self.theta * out_diff\n",
    "\n",
    "            else:\n",
    "                return out_normal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# stem_3DCNN + ST-ViT with local Depthwise Spatio-Temporal MLP\n",
    "class ViT_ST_ST_Compact3_TDC_gra_sharp(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        name: Optional[str] = None, \n",
    "        pretrained: bool = False, \n",
    "        patches: int = 16,\n",
    "        dim: int = 768,\n",
    "        ff_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        attention_dropout_rate: float = 0.0,\n",
    "        dropout_rate: float = 0.2,\n",
    "        representation_size: Optional[int] = None,\n",
    "        load_repr_layer: bool = False,\n",
    "        classifier: str = 'token',\n",
    "        #positional_embedding: str = '1d',\n",
    "        in_channels: int = 3, \n",
    "        frame: int = 160,\n",
    "        theta: float = 0.2,\n",
    "        image_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.image_size = image_size  \n",
    "        self.frame = frame  \n",
    "        self.dim = dim              \n",
    "\n",
    "        # Image and patch sizes\n",
    "        t, h, w = as_tuple(image_size)  # tube sizes\n",
    "        ft, fh, fw = as_tuple(patches)  # patch sizes, ft = 4 ==> 160/4=40\n",
    "        gt, gh, gw = t//ft, h // fh, w // fw  # number of patches\n",
    "        seq_len = gh * gw * gt\n",
    "\n",
    "        # Patch embedding    [4x16x16]conv\n",
    "        self.patch_embedding = nn.Conv3d(dim, dim, kernel_size=(ft, fh, fw), stride=(ft, fh, fw))\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer1 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//3, dim=dim, num_heads=num_heads, \n",
    "                                       ff_dim=ff_dim, dropout=dropout_rate, theta=theta)\n",
    "        # Transformer\n",
    "        self.transformer2 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//3, dim=dim, num_heads=num_heads, \n",
    "                                       ff_dim=ff_dim, dropout=dropout_rate, theta=theta)\n",
    "        # Transformer\n",
    "        self.transformer3 = Transformer_ST_TDC_gra_sharp(num_layers=num_layers//3, dim=dim, num_heads=num_heads, \n",
    "                                       ff_dim=ff_dim, dropout=dropout_rate, theta=theta)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Stem0 = nn.Sequential(\n",
    "            nn.Conv3d(3, dim//4, [1, 5, 5], stride=1, padding=[0,2,2]),\n",
    "            nn.BatchNorm3d(dim//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        self.Stem1 = nn.Sequential(\n",
    "            nn.Conv3d(dim//4, dim//2, [3, 3, 3], stride=1, padding=1),\n",
    "            nn.BatchNorm3d(dim//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        self.Stem2 = nn.Sequential(\n",
    "            nn.Conv3d(dim//2, dim, [3, 3, 3], stride=1, padding=1),\n",
    "            nn.BatchNorm3d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        #self.normLast = nn.LayerNorm(dim, eps=1e-6)\n",
    "        \n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=(8,1,1)),\n",
    "            nn.Conv3d(dim, dim, [3, 1, 1], stride=1, padding=(1,0,0)),   \n",
    "            nn.BatchNorm3d(dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=(5,1,1)),\n",
    "            nn.Conv3d(dim, dim//2, [3, 1, 1], stride=1, padding=(1,0,0)),   \n",
    "            nn.BatchNorm3d(dim//2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    " \n",
    "        self.ConvBlockLast = nn.Conv1d(dim//2, 1, 1,stride=1, padding=0)\n",
    "        \n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def init_weights(self):\n",
    "        def _init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # _trunc_normal(m.weight, std=0.02)  # from .initialization import _trunc_normal\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)  # nn.init.constant(m.bias, 0)\n",
    "        self.apply(_init)\n",
    "\n",
    "\n",
    "    def forward(self, x, gra_sharp):\n",
    "        b, c, t, fh, fw = x.shape\n",
    "        \n",
    "        x = self.Stem0(x)\n",
    "        x = self.Stem1(x)\n",
    "        x = self.Stem2(x)  # [B, 64, 160, 64, 64]\n",
    "        x = self.patch_embedding(x)  # [B, 64, 40, 4, 4]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, 40*4*4, 64]\n",
    "        \n",
    "        Trans_features, Score1 =  self.transformer1(x, gra_sharp)  # [B, 4*4*40, 64]\n",
    "        Trans_features2, Score2 =  self.transformer2(Trans_features, gra_sharp)  # [B, 4*4*40, 64]\n",
    "        Trans_features3, Score3 =  self.transformer3(Trans_features2, gra_sharp)  # [B, 4*4*40, 64]\n",
    "        \n",
    "        \n",
    "        #Trans_features3 = self.normLast(Trans_features3)\n",
    "        \n",
    "        # upsampling heads\n",
    "        #features_last = Trans_features3.transpose(1, 2).view(b, self.dim, 40, 4, 4) # [B, 64, 40, 4, 4]\n",
    "        features_last = Trans_features3.transpose(1, 2).view(b, self.dim, 4, 5,2) # [B, 64, 40, 4, 4]\n",
    "        \n",
    "        features_last = self.upsample(features_last)\t\t    # x [B, 64, 7*7, 80]\n",
    "        features_last = self.upsample2(features_last)\t\t    # x [B, 32, 7*7, 160]\n",
    "\n",
    "        features_last = torch.mean(features_last,3)     # x [B, 32, 160, 4]  \n",
    "        features_last = torch.mean(features_last,3)     # x [B, 32, 160]    \n",
    "        rPPG = self.ConvBlockLast(features_last)    # x [B, 1, 160]\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        rPPG = rPPG.squeeze(1)\n",
    "        \n",
    "        #return rPPG, Score1, Score2, Score3\n",
    "        return rPPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99c4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst,bvp, n):\n",
    "    tmp = []\n",
    "    tmp_bvp = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        tmp.append(torch.as_tensor(lst[i:i + n]))\n",
    "        tmp_bvp.append(torch.as_tensor(bvp[i:i + n]))\n",
    "    if tmp[-1].shape[0] != tmp[0].shape[0]:\n",
    "        return(torch.stack(tmp[:-1]),torch.stack(tmp_bvp[:-1]))\n",
    "    return(torch.stack(tmp),torch.stack(tmp_bvp))\n",
    "        \n",
    "#webs,train_bvp = chunks(webs,train_bvp,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e14710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tot):\n",
    "    min_len = 0\n",
    "    train_video = []\n",
    "    train_bvp = []\n",
    "    val_video = []\n",
    "    val_bvp = []\n",
    "    test_video = []\n",
    "    test_bvp = []\n",
    "    for idx in range(0,tot):\n",
    "        with open('/var/datasets/PURE_webs/'+str(idx)+'-WEBS-'+str(PATCH_SIZE), 'rb') as f:\n",
    "            #print('/var/datasets/PURE_webs/'+str(idx)+'-WEBS-'+str(PATCH_SIZE))\n",
    "            (webs,labels) = pickle.load(f)\n",
    "            if idx < (tot/10)*9:   #9 : 0 : 1\n",
    "                #print(1)\n",
    "                train_video.append(webs)\n",
    "                train_bvp.append(labels)\n",
    "            #elif idx < (tot/10)*8+(tot/10):\n",
    "                #print(2)\n",
    "                #val_video.append(webs)\n",
    "                #val_bvp.append(labels)\n",
    "            else:\n",
    "                #print(3)\n",
    "                test_video.append(webs)\n",
    "                test_bvp.append(labels)\n",
    "            if min_len==0 or len(webs)<min_len:\n",
    "                min_len = len(webs)\n",
    "    for i in range(0,len(train_video)):\n",
    "        train_video[i],train_bvp[i] = chunks(train_video[i][:min_len-1], train_bvp[i][:min_len-1],160)\n",
    "    #for i in range(0,len(val_video)):\n",
    "    #    val_video[i],val_bvp[i] = chunks(val_video[i][:min_len-1], val_bvp[i][:min_len-1],160)\n",
    "    for i in range(0,len(test_video)):\n",
    "        test_video[i],test_bvp[i] = chunks(test_video[i][:min_len-1], test_bvp[i][:min_len-1],160)\n",
    "    return (train_video, train_bvp), (val_video, val_bvp), (test_video, test_bvp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7adaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(model, optimizer, epoch, loss, iteration, path=\".\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'iteration': iteration + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    f_path = path + '/checkpoint.pt'\n",
    "    torch.save(checkpoint, f_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28999bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(model, optimizer, path='./checkpoint.pt'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch'], checkpoint['iteration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e27886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neg_Pearson(nn.Module):    # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss                                                                            \n",
    "    def __init__(self):                                                                                                                                                   \n",
    "        super(Neg_Pearson,self).__init__()                                                                                                                                \n",
    "        return\n",
    "    def forward(self, preds, labels):       # all variable operation    \n",
    "        loss = 0                                                                                                                                                          \n",
    "        for i in range(preds.shape[0]):                                                                                                                                   \n",
    "            sum_x = torch.sum(preds[i])                # x                                                                                                                \n",
    "            sum_y = torch.sum(labels[i])               # y                                                                                                                \n",
    "            sum_xy = torch.sum(preds[i]*labels[i])        # xy                                                                                                            \n",
    "            sum_x2 = torch.sum(torch.pow(preds[i],2))  # x^2                                                                                                              \n",
    "            sum_y2 = torch.sum(torch.pow(labels[i],2)) # y^2                                                                                                              \n",
    "            N = preds.shape[1]                                                                                                                                            \n",
    "            pearson = (N*sum_xy - sum_x*sum_y)/(torch.sqrt((N*sum_x2 - torch.pow(sum_x,2))*(N*sum_y2 - torch.pow(sum_y,2))))                                              \n",
    "\n",
    "            loss += 1 - pearson                                                                                                                                           \n",
    "\n",
    "        loss = loss/preds.shape[0]                                                                                                                                        \n",
    "        return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b15612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da28c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_video,train_bvp),(val_video,val_bvp),(test_video,test_bvp) = load_data(len(allvideo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24ac2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video, train_bvp = torch.cat(train_video[:]), torch.cat(train_bvp[:])\n",
    "#val_video, val_bvp     = torch.cat(val_video[:]), torch.cat(val_bvp[:])\n",
    "test_video, test_bvp   = torch.cat(test_video[:]), torch.cat(test_bvp[:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f1a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 160, 160, 160, 3]) torch.Size([594, 160])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                                            Param #\n",
      "==========================================================================================\n",
      "ViT_ST_ST_Compact3_TDC_gra_sharp                                  --\n",
      "├─Conv3d: 1-1                                                     26,214,560\n",
      "├─Transformer_ST_TDC_gra_sharp: 1-2                               --\n",
      "│    └─ModuleList: 2-1                                            --\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-1                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-2                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-3                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-4                           1,485,904\n",
      "├─Transformer_ST_TDC_gra_sharp: 1-3                               --\n",
      "│    └─ModuleList: 2-2                                            --\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-5                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-6                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-7                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-8                           1,485,904\n",
      "├─Transformer_ST_TDC_gra_sharp: 1-4                               --\n",
      "│    └─ModuleList: 2-3                                            --\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-9                           1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-10                          1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-11                          1,485,904\n",
      "│    │    └─Block_ST_TDC_gra_sharp: 3-12                          1,485,904\n",
      "├─Sequential: 1-5                                                 --\n",
      "│    └─Conv3d: 2-4                                                3,040\n",
      "│    └─BatchNorm3d: 2-5                                           80\n",
      "│    └─ReLU: 2-6                                                  --\n",
      "│    └─MaxPool3d: 2-7                                             --\n",
      "├─Sequential: 1-6                                                 --\n",
      "│    └─Conv3d: 2-8                                                86,480\n",
      "│    └─BatchNorm3d: 2-9                                           160\n",
      "│    └─ReLU: 2-10                                                 --\n",
      "│    └─MaxPool3d: 2-11                                            --\n",
      "├─Sequential: 1-7                                                 --\n",
      "│    └─Conv3d: 2-12                                               345,760\n",
      "│    └─BatchNorm3d: 2-13                                          320\n",
      "│    └─ReLU: 2-14                                                 --\n",
      "│    └─MaxPool3d: 2-15                                            --\n",
      "├─Sequential: 1-8                                                 --\n",
      "│    └─Upsample: 2-16                                             --\n",
      "│    └─Conv3d: 2-17                                               76,960\n",
      "│    └─BatchNorm3d: 2-18                                          320\n",
      "│    └─ELU: 2-19                                                  --\n",
      "├─Sequential: 1-9                                                 --\n",
      "│    └─Upsample: 2-20                                             --\n",
      "│    └─Conv3d: 2-21                                               38,480\n",
      "│    └─BatchNorm3d: 2-22                                          160\n",
      "│    └─ELU: 2-23                                                  --\n",
      "├─Conv1d: 1-10                                                    81\n",
      "==========================================================================================\n",
      "Total params: 44,597,249\n",
      "Trainable params: 44,597,249\n",
      "Non-trainable params: 0\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "BATCH = 4\n",
    "#train_video = train_video.permute(0,4,1,2,3)\n",
    "#val_video = val_video.permute(0,4,1,2,3)\n",
    "#test_video = test_video.permute(0,4,1,2,3)\n",
    "print(train_video.shape,train_bvp.shape)\n",
    "dataset = torch.utils.data.TensorDataset(train_video.permute(0,4,1,2,3),torch.as_tensor(train_bvp))\n",
    "trainloader = torch.utils.data.DataLoader(dataset,batch_size=BATCH, shuffle=True, num_workers=1)\n",
    "\n",
    "model = ViT_ST_ST_Compact3_TDC_gra_sharp(image_size=(160,160,160), patches=(4,16,16), dim=160, ff_dim=144, num_heads=4, num_layers=12, dropout_rate=0.1, theta=0.7)\n",
    "print(summary(model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.L1Loss()\n",
    "criterion_Pearson = Neg_Pearson() \n",
    "\n",
    "loss = 0.0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54552d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, trainloader, epoch_start=0, iter_start=0):\n",
    "    criterion_reg = nn.MSELoss()\n",
    "    criterion_L1loss = nn.L1Loss()\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_Pearson = Neg_Pearson()\n",
    "    \n",
    "    a_start = 0.1\n",
    "    b_start = 1.0\n",
    "    exp_a = 0.5\n",
    "    exp_b = 5.0\n",
    "    \n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        print(\"\\nStarting epoch\", epoch+1)\n",
    "        current_loss = 0.0\n",
    "        loss = 0.0\n",
    "        loss_rPPG_avg = AvgrageMeter()\n",
    "        loss_peak_avg = AvgrageMeter()\n",
    "        loss_kl_avg_test = AvgrageMeter()\n",
    "        loss_bvp_mae = AvgrageMeter()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i, data in enumerate(trainloader,0):\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(f\"Iteration {i+1}, {loss_rPPG_avg.avg:1.5f}, {loss_peak_avg.avg:1.5f}, {loss_kl_avg_test.avg:1.5f}\")\n",
    "            sys.stdout.flush()\n",
    "            if i >= iter_start: \n",
    "                iter_start = 0\n",
    "                inputs, targets = data\n",
    "                inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "                optimizer.zero_grad()\n",
    "                rPPG = model(inputs,0.2)   \n",
    "                rPPG = (rPPG-torch.mean(rPPG)) /torch.std(rPPG)\n",
    "                #loss\n",
    "                loss_rPPG = criterion_Pearson(rPPG, targets)\n",
    "                fre_loss = 0.0\n",
    "                kl_loss = 0.0\n",
    "                train_mae = 0.0\n",
    "                for bb in range(BATCH):\n",
    "                    loss_distribution_kl, fre_loss_temp, train_mae_temp = TorchLossComputer.cross_entropy_power_spectrum_DLDL_softmax2(\n",
    "                        rPPG[bb], torch.mean(targets[bb].float()), 30, std=1.0)  # std=1.1\n",
    "                    fre_loss = fre_loss + fre_loss_temp\n",
    "                    kl_loss = kl_loss + loss_distribution_kl\n",
    "                    train_mae = train_mae + train_mae_temp\n",
    "                fre_loss = fre_loss/BATCH\n",
    "                kl_loss = kl_loss/BATCH\n",
    "                train_mae = train_mae/BATCH\n",
    "                if epoch >25:\n",
    "                    a = 0.05\n",
    "                    b = 5.0\n",
    "                else:\n",
    "                    a = a_start*math.pow(exp_a, epoch/25.0)\n",
    "                    b = b_start*math.pow(exp_b, epoch/25.0)\n",
    "            \n",
    "                a = 0.1\n",
    "                #b = 1.0\n",
    "            \n",
    "                loss =  a*loss_rPPG + b*(fre_loss+kl_loss)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_rPPG_avg.update(loss_rPPG.data, BATCH)\n",
    "                loss_peak_avg.update(fre_loss.data, BATCH)\n",
    "                loss_kl_avg_test.update(kl_loss.data, BATCH)\n",
    "                loss_bvp_mae.update(train_mae, BATCH)\n",
    "                save_ckp(model, optimizer, loss, epoch, i)\n",
    "            \n",
    "                '''\n",
    "                outputs = torch.mean(outputs,1)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                current_loss += loss.item()\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write(f\"Iteration {i}, Loss:  {current_loss/BATCH:1.5f}\")\n",
    "                sys.stdout.flush()\n",
    "                if i % 10 == 0:\n",
    "                    last_loss = current_loss\n",
    "                    current_loss = 0.0'''\n",
    "\n",
    "    return model, loss, loss_rPPG_avg, loss_peak_avg, loss_kl_avg_test, loss_bvp_mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693f0bec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations per epoch:  149\n",
      "epochs: 5\n",
      "Start\n",
      "\n",
      "Starting epoch 1\n",
      "\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2800809/3965833656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations per epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epochs: {0}\\nStart\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTraining process has finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2800809/877251966.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, trainloader, epoch_start, iter_start)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {i+1}, {loss_rPPG_avg.avg:1.5f}, {loss_peak_avg.avg:1.5f}, {loss_kl_avg_test.avg:1.5f}, {loss_bvp_mae.avg[0]:1.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miter_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = ViT_ST_ST_Compact3_TDC_gra_sharp(image_size=(160,160,160), patches=(4,16,16), dim=160, ff_dim=144, num_heads=4, num_layers=12, dropout_rate=0.1, theta=0.7)\n",
    "model = model.cuda()\n",
    "model.train() \n",
    "print(\"iterations per epoch: \",len(trainloader)) \n",
    "print(\"epochs: {0}\\nStart\".format(epochs)) \n",
    "train(model,optimizer,trainloader, 0,0) \n",
    "print('\\nTraining process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1ed652c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 7989),\n",
       "             ('active.all.current', 1392),\n",
       "             ('active.all.freed', 6597),\n",
       "             ('active.all.peak', 1964),\n",
       "             ('active.large_pool.allocated', 879),\n",
       "             ('active.large_pool.current', 118),\n",
       "             ('active.large_pool.freed', 761),\n",
       "             ('active.large_pool.peak', 122),\n",
       "             ('active.small_pool.allocated', 7110),\n",
       "             ('active.small_pool.current', 1274),\n",
       "             ('active.small_pool.freed', 5836),\n",
       "             ('active.small_pool.peak', 1844),\n",
       "             ('active_bytes.all.allocated', 50750545920),\n",
       "             ('active_bytes.all.current', 13758904320),\n",
       "             ('active_bytes.all.freed', 36991641600),\n",
       "             ('active_bytes.all.peak', 13994092032),\n",
       "             ('active_bytes.large_pool.allocated', 50174035968),\n",
       "             ('active_bytes.large_pool.current', 13733990400),\n",
       "             ('active_bytes.large_pool.freed', 36440045568),\n",
       "             ('active_bytes.large_pool.peak', 13981298688),\n",
       "             ('active_bytes.small_pool.allocated', 576509952),\n",
       "             ('active_bytes.small_pool.current', 24913920),\n",
       "             ('active_bytes.small_pool.freed', 551596032),\n",
       "             ('active_bytes.small_pool.peak', 55501312),\n",
       "             ('allocated_bytes.all.allocated', 50750545920),\n",
       "             ('allocated_bytes.all.current', 13758904320),\n",
       "             ('allocated_bytes.all.freed', 36991641600),\n",
       "             ('allocated_bytes.all.peak', 13994092032),\n",
       "             ('allocated_bytes.large_pool.allocated', 50174035968),\n",
       "             ('allocated_bytes.large_pool.current', 13733990400),\n",
       "             ('allocated_bytes.large_pool.freed', 36440045568),\n",
       "             ('allocated_bytes.large_pool.peak', 13981298688),\n",
       "             ('allocated_bytes.small_pool.allocated', 576509952),\n",
       "             ('allocated_bytes.small_pool.current', 24913920),\n",
       "             ('allocated_bytes.small_pool.freed', 551596032),\n",
       "             ('allocated_bytes.small_pool.peak', 55501312),\n",
       "             ('allocation.all.allocated', 7989),\n",
       "             ('allocation.all.current', 1392),\n",
       "             ('allocation.all.freed', 6597),\n",
       "             ('allocation.all.peak', 1964),\n",
       "             ('allocation.large_pool.allocated', 879),\n",
       "             ('allocation.large_pool.current', 118),\n",
       "             ('allocation.large_pool.freed', 761),\n",
       "             ('allocation.large_pool.peak', 122),\n",
       "             ('allocation.small_pool.allocated', 7110),\n",
       "             ('allocation.small_pool.current', 1274),\n",
       "             ('allocation.small_pool.freed', 5836),\n",
       "             ('allocation.small_pool.peak', 1844),\n",
       "             ('inactive_split.all.allocated', 3684),\n",
       "             ('inactive_split.all.current', 59),\n",
       "             ('inactive_split.all.freed', 3625),\n",
       "             ('inactive_split.all.peak', 70),\n",
       "             ('inactive_split.large_pool.allocated', 370),\n",
       "             ('inactive_split.large_pool.current', 8),\n",
       "             ('inactive_split.large_pool.freed', 362),\n",
       "             ('inactive_split.large_pool.peak', 11),\n",
       "             ('inactive_split.small_pool.allocated', 3314),\n",
       "             ('inactive_split.small_pool.current', 51),\n",
       "             ('inactive_split.small_pool.freed', 3263),\n",
       "             ('inactive_split.small_pool.peak', 59),\n",
       "             ('inactive_split_bytes.all.allocated', 14353026048),\n",
       "             ('inactive_split_bytes.all.current', 224905216),\n",
       "             ('inactive_split_bytes.all.freed', 14128120832),\n",
       "             ('inactive_split_bytes.all.peak', 2682089984),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 13726459904),\n",
       "             ('inactive_split_bytes.large_pool.current', 199487488),\n",
       "             ('inactive_split_bytes.large_pool.freed', 13526972416),\n",
       "             ('inactive_split_bytes.large_pool.peak', 2669246464),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 626566144),\n",
       "             ('inactive_split_bytes.small_pool.current', 25417728),\n",
       "             ('inactive_split_bytes.small_pool.freed', 601148416),\n",
       "             ('inactive_split_bytes.small_pool.peak', 25422848),\n",
       "             ('max_split_size', -1),\n",
       "             ('num_alloc_retries', 12),\n",
       "             ('num_ooms', 7),\n",
       "             ('oversize_allocations.allocated', 0),\n",
       "             ('oversize_allocations.current', 0),\n",
       "             ('oversize_allocations.freed', 0),\n",
       "             ('oversize_allocations.peak', 0),\n",
       "             ('oversize_segments.allocated', 0),\n",
       "             ('oversize_segments.current', 0),\n",
       "             ('oversize_segments.freed', 0),\n",
       "             ('oversize_segments.peak', 0),\n",
       "             ('reserved_bytes.all.allocated', 23479713792),\n",
       "             ('reserved_bytes.all.current', 13983809536),\n",
       "             ('reserved_bytes.all.freed', 9495904256),\n",
       "             ('reserved_bytes.all.peak', 14180941824),\n",
       "             ('reserved_bytes.large_pool.allocated', 23420993536),\n",
       "             ('reserved_bytes.large_pool.current', 13933477888),\n",
       "             ('reserved_bytes.large_pool.freed', 9487515648),\n",
       "             ('reserved_bytes.large_pool.peak', 14130610176),\n",
       "             ('reserved_bytes.small_pool.allocated', 58720256),\n",
       "             ('reserved_bytes.small_pool.current', 50331648),\n",
       "             ('reserved_bytes.small_pool.freed', 8388608),\n",
       "             ('reserved_bytes.small_pool.peak', 56623104),\n",
       "             ('segment.all.allocated', 59),\n",
       "             ('segment.all.current', 41),\n",
       "             ('segment.all.freed', 18),\n",
       "             ('segment.all.peak', 44),\n",
       "             ('segment.large_pool.allocated', 31),\n",
       "             ('segment.large_pool.current', 17),\n",
       "             ('segment.large_pool.freed', 14),\n",
       "             ('segment.large_pool.peak', 22),\n",
       "             ('segment.small_pool.allocated', 28),\n",
       "             ('segment.small_pool.current', 24),\n",
       "             ('segment.small_pool.freed', 4),\n",
       "             ('segment.small_pool.peak', 27)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fe87ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epoch_loss.sv', 'rb') as f:\n",
    "    loss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f081310b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02524162456393242"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss[-1][4]/len(dataset)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1\n",
      "Iteration 77, 0.22911"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model = ViT_ST_ST_Compact3_TDC_gra_sharp(image_size=(160,160,160), patches=(4,16,16), dim=160, ff_dim=144, num_heads=4, num_layers=12, dropout_rate=0.1, theta=0.7)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model, optimizer, epoch, iteration = load_ckp(model,optimizer)\n",
    "train(model,optimizer, trainloader, epoch, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60046908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1947, 150, 150, 3]) torch.Size([1947])\n"
     ]
    }
   ],
   "source": [
    "#train_bvp = signal.resample(sigGT.data, len(webs))\n",
    "print(webs.shape,torch.as_tensor(train_bvp).shape)\n",
    "dataset = torch.utils.data.TensorDataset(webs,torch.as_tensor(train_bvp))\n",
    "trainloader = torch.utils.data.DataLoader(dataset, shuffle=True, num_workers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd398866",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Accuracy: 12.54637 tensor([[33.3883, 33.4594, 33.4671, 33.4381, 33.4450, 33.4685, 33.4702, 33.4694,\n",
      "         33.4371, 33.4689, 33.4699, 33.4692, 33.4323, 33.4369, 33.4478, 33.4447,\n",
      "         33.4683, 33.4036, 33.4307, 33.4642, 33.4683, 33.4700, 33.4674, 33.4713,\n",
      "         33.4649, 33.4346, 33.4710, 33.4680, 33.4678, 33.4389, 33.4655, 33.4705,\n",
      "         33.4475, 33.4087, 33.4680, 33.4308, 33.4707, 33.4556, 33.4609, 33.4681,\n",
      "         33.4681, 33.4705, 33.4030, 33.4583, 33.4490, 33.4658, 33.4659, 33.4619,\n",
      "         33.4395, 33.4692, 33.4637, 33.4693, 33.4654, 33.4597, 33.4446, 33.4483,\n",
      "         33.4561, 33.4448, 33.4606, 33.4654, 33.4650, 33.4665, 33.4690, 33.4684,\n",
      "         33.4694, 33.4434, 33.4625, 33.4686, 33.4012, 33.4627, 33.4681, 33.4530,\n",
      "         33.4712, 33.4235, 33.4270, 33.4539, 33.4641, 33.4694, 33.4646, 33.4666,\n",
      "         33.4708, 33.4479, 33.4219, 33.4604, 33.4122, 33.4586, 33.4699, 33.4640,\n",
      "         33.4649, 33.4517, 33.4699, 33.4114, 33.4297, 33.4703, 33.4051, 33.4590,\n",
      "Iteration 2, Accuracy: 11.50019 tensor([[33.3883, 33.4583, 33.4678, 33.4369, 33.4449, 33.4685, 33.4706, 33.4697,\n",
      "         33.4329, 33.4691, 33.4697, 33.4697, 33.4312, 33.4341, 33.4483, 33.4472,\n",
      "         33.4678, 33.4072, 33.4301, 33.4649, 33.4673, 33.4701, 33.4675, 33.4710,\n",
      "         33.4654, 33.4364, 33.4709, 33.4681, 33.4680, 33.4397, 33.4653, 33.4706,\n",
      "         33.4497, 33.4073, 33.4678, 33.4349, 33.4706, 33.4548, 33.4584, 33.4681,\n",
      "         33.4659, 33.4706, 33.4021, 33.4619, 33.4483, 33.4664, 33.4679, 33.4618,\n",
      "         33.4396, 33.4690, 33.4636, 33.4693, 33.4657, 33.4600, 33.4472, 33.4489,\n",
      "         33.4601, 33.4441, 33.4608, 33.4649, 33.4654, 33.4668, 33.4691, 33.4682,\n",
      "         33.4695, 33.4442, 33.4665, 33.4688, 33.4012, 33.4630, 33.4683, 33.4515,\n",
      "         33.4709, 33.4255, 33.4322, 33.4556, 33.4659, 33.4696, 33.4645, 33.4662,\n",
      "         33.4706, 33.4482, 33.4193, 33.4592, 33.4247, 33.4603, 33.4702, 33.4636,\n",
      "         33.4656, 33.4515, 33.4697, 33.4094, 33.4341, 33.4704, 33.4047, 33.4581,\n",
      "Iteration 3, Accuracy: 7.84886 tensor([[33.3883, 33.4602, 33.4668, 33.4379, 33.4448, 33.4685, 33.4705, 33.4695,\n",
      "         33.4388, 33.4688, 33.4700, 33.4697, 33.4324, 33.4373, 33.4478, 33.4433,\n",
      "         33.4681, 33.4033, 33.4301, 33.4640, 33.4683, 33.4700, 33.4675, 33.4713,\n",
      "         33.4649, 33.4375, 33.4710, 33.4681, 33.4678, 33.4384, 33.4656, 33.4703,\n",
      "         33.4491, 33.4087, 33.4679, 33.4291, 33.4707, 33.4554, 33.4608, 33.4681,\n",
      "         33.4678, 33.4705, 33.4027, 33.4596, 33.4489, 33.4658, 33.4658, 33.4621,\n",
      "         33.4396, 33.4692, 33.4637, 33.4694, 33.4653, 33.4597, 33.4444, 33.4491,\n",
      "         33.4553, 33.4500, 33.4600, 33.4651, 33.4652, 33.4668, 33.4690, 33.4682,\n",
      "         33.4695, 33.4490, 33.4645, 33.4687, 33.4011, 33.4624, 33.4682, 33.4529,\n",
      "         33.4711, 33.4237, 33.4255, 33.4521, 33.4648, 33.4691, 33.4645, 33.4668,\n",
      "         33.4709, 33.4476, 33.4228, 33.4607, 33.4175, 33.4585, 33.4701, 33.4641,\n",
      "         33.4650, 33.4517, 33.4699, 33.4127, 33.4293, 33.4703, 33.4052, 33.4589,\n",
      "Iteration 4, Accuracy: 8.77329 tensor([[33.3883, 33.4599, 33.4667, 33.4392, 33.4450, 33.4693, 33.4710, 33.4698,\n",
      "         33.4349, 33.4690, 33.4701, 33.4697, 33.4317, 33.4337, 33.4468, 33.4423,\n",
      "         33.4687, 33.4081, 33.4318, 33.4645, 33.4676, 33.4700, 33.4683, 33.4713,\n",
      "         33.4646, 33.4319, 33.4711, 33.4684, 33.4684, 33.4388, 33.4657, 33.4707,\n",
      "         33.4456, 33.4081, 33.4681, 33.4301, 33.4708, 33.4549, 33.4585, 33.4685,\n",
      "         33.4680, 33.4707, 33.4038, 33.4609, 33.4483, 33.4665, 33.4663, 33.4624,\n",
      "         33.4394, 33.4687, 33.4640, 33.4694, 33.4655, 33.4553, 33.4378, 33.4469,\n",
      "         33.4543, 33.4502, 33.4613, 33.4656, 33.4653, 33.4671, 33.4692, 33.4690,\n",
      "         33.4697, 33.4469, 33.4649, 33.4691, 33.4010, 33.4631, 33.4683, 33.4526,\n",
      "         33.4711, 33.4217, 33.4242, 33.4478, 33.4644, 33.4692, 33.4624, 33.4673,\n",
      "         33.4709, 33.4475, 33.4191, 33.4619, 33.4081, 33.4606, 33.4700, 33.4645,\n",
      "         33.4656, 33.4505, 33.4698, 33.4095, 33.4258, 33.4706, 33.4071, 33.4615,\n",
      "Iteration 5, Accuracy: 10.92751 tensor([[33.3881, 33.4582, 33.4713, 33.4395, 33.4460, 33.4685, 33.4713, 33.4705,\n",
      "         33.4350, 33.4704, 33.4696, 33.4690, 33.4522, 33.4439, 33.4585, 33.4499,\n",
      "         33.4699, 33.4089, 33.4306, 33.4665, 33.4664, 33.4703, 33.4660, 33.4715,\n",
      "         33.4700, 33.4429, 33.4687, 33.4688, 33.4695, 33.4393, 33.4660, 33.4709,\n",
      "         33.4516, 33.4226, 33.4668, 33.4390, 33.4679, 33.4518, 33.4595, 33.4695,\n",
      "         33.4676, 33.4711, 33.4221, 33.4568, 33.4599, 33.4686, 33.4713, 33.4622,\n",
      "         33.4420, 33.4695, 33.4627, 33.4676, 33.4636, 33.4456, 33.4348, 33.4483,\n",
      "         33.4702, 33.4511, 33.4621, 33.4626, 33.4642, 33.4657, 33.4683, 33.4680,\n",
      "         33.4699, 33.4457, 33.4712, 33.4697, 33.4047, 33.4519, 33.4670, 33.4508,\n",
      "         33.4703, 33.4255, 33.4496, 33.4692, 33.4701, 33.4697, 33.4596, 33.4673,\n",
      "         33.4708, 33.4453, 33.4223, 33.4656, 33.4315, 33.4577, 33.4701, 33.4643,\n",
      "         33.4691, 33.4490, 33.4694, 33.4141, 33.4278, 33.4704, 33.4310, 33.4625,\n",
      "Iteration 6, Accuracy: 9.36389 tensor([[33.3882, 33.4580, 33.4671, 33.4380, 33.4449, 33.4686, 33.4704, 33.4697,\n",
      "         33.4359, 33.4690, 33.4698, 33.4693, 33.4275, 33.4358, 33.4483, 33.4449,\n",
      "         33.4683, 33.4091, 33.4315, 33.4645, 33.4674, 33.4700, 33.4675, 33.4713,\n",
      "         33.4658, 33.4419, 33.4710, 33.4681, 33.4680, 33.4384, 33.4654, 33.4706,\n",
      "         33.4509, 33.4071, 33.4676, 33.4372, 33.4706, 33.4553, 33.4608, 33.4684,\n",
      "         33.4678, 33.4706, 33.4009, 33.4610, 33.4486, 33.4661, 33.4680, 33.4614,\n",
      "         33.4414, 33.4696, 33.4638, 33.4694, 33.4660, 33.4606, 33.4492, 33.4476,\n",
      "         33.4595, 33.4474, 33.4604, 33.4652, 33.4653, 33.4664, 33.4691, 33.4684,\n",
      "         33.4695, 33.4444, 33.4660, 33.4688, 33.4022, 33.4623, 33.4685, 33.4529,\n",
      "         33.4711, 33.4232, 33.4313, 33.4557, 33.4652, 33.4695, 33.4636, 33.4661,\n",
      "         33.4706, 33.4449, 33.4216, 33.4601, 33.4241, 33.4605, 33.4699, 33.4637,\n",
      "         33.4653, 33.4498, 33.4698, 33.4104, 33.4358, 33.4705, 33.4045, 33.4581,\n",
      "Iteration 7, Accuracy: 11.67563 tensor([[33.3883, 33.4612, 33.4677, 33.4389, 33.4462, 33.4692, 33.4706, 33.4696,\n",
      "         33.4366, 33.4691, 33.4701, 33.4698, 33.4353, 33.4422, 33.4487, 33.4444,\n",
      "         33.4700, 33.4015, 33.4203, 33.4657, 33.4688, 33.4699, 33.4676, 33.4715,\n",
      "         33.4677, 33.4316, 33.4705, 33.4685, 33.4687, 33.4388, 33.4659, 33.4704,\n",
      "         33.4505, 33.4153, 33.4695, 33.4272, 33.4703, 33.4563, 33.4605, 33.4684,\n",
      "         33.4664, 33.4703, 33.4080, 33.4612, 33.4522, 33.4665, 33.4692, 33.4619,\n",
      "         33.4389, 33.4688, 33.4640, 33.4686, 33.4649, 33.4599, 33.4453, 33.4435,\n",
      "         33.4560, 33.4479, 33.4614, 33.4640, 33.4649, 33.4659, 33.4692, 33.4689,\n",
      "         33.4697, 33.4475, 33.4652, 33.4689, 33.3960, 33.4600, 33.4680, 33.4517,\n",
      "         33.4711, 33.4260, 33.4272, 33.4584, 33.4663, 33.4695, 33.4615, 33.4669,\n",
      "         33.4709, 33.4443, 33.4231, 33.4634, 33.4120, 33.4592, 33.4703, 33.4644,\n",
      "         33.4664, 33.4478, 33.4690, 33.4193, 33.4290, 33.4699, 33.4094, 33.4606,\n",
      "Iteration 8, Accuracy: 10.77315 tensor([[33.3880, 33.4567, 33.4712, 33.4394, 33.4462, 33.4682, 33.4713, 33.4706,\n",
      "         33.4375, 33.4704, 33.4690, 33.4693, 33.4478, 33.4397, 33.4587, 33.4502,\n",
      "         33.4701, 33.4144, 33.4349, 33.4668, 33.4670, 33.4701, 33.4656, 33.4711,\n",
      "         33.4692, 33.4440, 33.4679, 33.4685, 33.4696, 33.4398, 33.4659, 33.4708,\n",
      "         33.4514, 33.4217, 33.4666, 33.4421, 33.4675, 33.4536, 33.4618, 33.4692,\n",
      "         33.4650, 33.4711, 33.4198, 33.4614, 33.4606, 33.4685, 33.4712, 33.4615,\n",
      "         33.4405, 33.4691, 33.4619, 33.4672, 33.4637, 33.4462, 33.4359, 33.4482,\n",
      "         33.4710, 33.4468, 33.4618, 33.4616, 33.4639, 33.4656, 33.4681, 33.4674,\n",
      "         33.4698, 33.4480, 33.4715, 33.4695, 33.4078, 33.4540, 33.4665, 33.4498,\n",
      "         33.4700, 33.4250, 33.4535, 33.4699, 33.4711, 33.4692, 33.4617, 33.4678,\n",
      "         33.4704, 33.4408, 33.4230, 33.4641, 33.4367, 33.4581, 33.4691, 33.4637,\n",
      "         33.4691, 33.4493, 33.4685, 33.4127, 33.4301, 33.4704, 33.4301, 33.4605,\n",
      "Iteration 9, Accuracy: 10.51566 tensor([[33.3881, 33.4578, 33.4712, 33.4414, 33.4498, 33.4675, 33.4714, 33.4701,\n",
      "         33.4343, 33.4696, 33.4700, 33.4708, 33.4574, 33.4581, 33.4618, 33.4483,\n",
      "         33.4690, 33.4111, 33.4284, 33.4674, 33.4665, 33.4694, 33.4652, 33.4710,\n",
      "         33.4697, 33.4461, 33.4690, 33.4701, 33.4690, 33.4391, 33.4638, 33.4702,\n",
      "         33.4525, 33.4366, 33.4661, 33.4379, 33.4689, 33.4510, 33.4602, 33.4693,\n",
      "         33.4686, 33.4705, 33.4239, 33.4591, 33.4632, 33.4648, 33.4713, 33.4636,\n",
      "         33.4440, 33.4633, 33.4656, 33.4683, 33.4667, 33.4343, 33.4290, 33.4460,\n",
      "         33.4698, 33.4415, 33.4632, 33.4500, 33.4667, 33.4649, 33.4688, 33.4661,\n",
      "         33.4700, 33.4492, 33.4711, 33.4700, 33.4004, 33.4446, 33.4695, 33.4530,\n",
      "         33.4698, 33.4319, 33.4487, 33.4696, 33.4712, 33.4694, 33.4610, 33.4688,\n",
      "         33.4710, 33.4454, 33.4205, 33.4686, 33.4327, 33.4640, 33.4704, 33.4659,\n",
      "         33.4664, 33.4502, 33.4695, 33.4181, 33.4191, 33.4677, 33.4388, 33.4626,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Accuracy: 10.00970 tensor([[33.3880, 33.4579, 33.4712, 33.4433, 33.4493, 33.4668, 33.4711, 33.4700,\n",
      "         33.4380, 33.4699, 33.4699, 33.4709, 33.4562, 33.4529, 33.4627, 33.4500,\n",
      "         33.4693, 33.4144, 33.4277, 33.4664, 33.4658, 33.4694, 33.4635, 33.4710,\n",
      "         33.4688, 33.4449, 33.4672, 33.4692, 33.4690, 33.4421, 33.4637, 33.4702,\n",
      "         33.4532, 33.4336, 33.4661, 33.4445, 33.4680, 33.4531, 33.4589, 33.4690,\n",
      "         33.4656, 33.4706, 33.4259, 33.4615, 33.4615, 33.4657, 33.4710, 33.4622,\n",
      "         33.4458, 33.4659, 33.4637, 33.4667, 33.4648, 33.4369, 33.4268, 33.4468,\n",
      "         33.4712, 33.4418, 33.4616, 33.4595, 33.4659, 33.4637, 33.4683, 33.4646,\n",
      "         33.4700, 33.4471, 33.4716, 33.4692, 33.4014, 33.4415, 33.4687, 33.4523,\n",
      "         33.4687, 33.4293, 33.4551, 33.4703, 33.4709, 33.4697, 33.4592, 33.4674,\n",
      "         33.4708, 33.4496, 33.4225, 33.4689, 33.4429, 33.4621, 33.4691, 33.4646,\n",
      "         33.4675, 33.4510, 33.4693, 33.4184, 33.4271, 33.4679, 33.4394, 33.4623,\n",
      "Iteration 11, Accuracy: 10.60366 tensor([[33.3880, 33.4602, 33.4708, 33.4467, 33.4521, 33.4656, 33.4700, 33.4694,\n",
      "         33.4513, 33.4604, 33.4686, 33.4706, 33.4645, 33.4643, 33.4713, 33.4523,\n",
      "         33.4689, 33.4234, 33.4272, 33.4682, 33.4636, 33.4675, 33.4625, 33.4688,\n",
      "         33.4674, 33.4504, 33.4667, 33.4699, 33.4691, 33.4581, 33.4629, 33.4689,\n",
      "         33.4543, 33.4495, 33.4638, 33.4474, 33.4672, 33.4426, 33.4615, 33.4687,\n",
      "         33.4672, 33.4691, 33.4241, 33.4583, 33.4692, 33.4635, 33.4708, 33.4660,\n",
      "         33.4605, 33.4541, 33.4675, 33.4673, 33.4683, 33.4245, 33.4263, 33.4509,\n",
      "         33.4715, 33.4420, 33.4656, 33.4467, 33.4687, 33.4648, 33.4687, 33.4625,\n",
      "         33.4710, 33.4477, 33.4715, 33.4701, 33.4047, 33.4159, 33.4702, 33.4533,\n",
      "         33.4674, 33.4349, 33.4640, 33.4709, 33.4710, 33.4700, 33.4572, 33.4701,\n",
      "         33.4692, 33.4454, 33.4198, 33.4715, 33.4516, 33.4687, 33.4690, 33.4675,\n",
      "         33.4673, 33.4545, 33.4665, 33.4088, 33.4204, 33.4637, 33.4495, 33.4664,\n",
      "Iteration 12, Accuracy: 10.34115 tensor([[33.3883, 33.4586, 33.4670, 33.4379, 33.4449, 33.4691, 33.4711, 33.4700,\n",
      "         33.4330, 33.4689, 33.4701, 33.4698, 33.4318, 33.4349, 33.4472, 33.4429,\n",
      "         33.4682, 33.4066, 33.4305, 33.4649, 33.4676, 33.4700, 33.4680, 33.4712,\n",
      "         33.4661, 33.4381, 33.4709, 33.4685, 33.4684, 33.4391, 33.4656, 33.4707,\n",
      "         33.4463, 33.4082, 33.4670, 33.4311, 33.4706, 33.4547, 33.4601, 33.4683,\n",
      "         33.4664, 33.4706, 33.4033, 33.4611, 33.4482, 33.4664, 33.4674, 33.4623,\n",
      "         33.4388, 33.4696, 33.4642, 33.4694, 33.4656, 33.4559, 33.4457, 33.4476,\n",
      "         33.4552, 33.4445, 33.4607, 33.4647, 33.4651, 33.4668, 33.4691, 33.4688,\n",
      "         33.4696, 33.4414, 33.4647, 33.4689, 33.4002, 33.4621, 33.4684, 33.4526,\n",
      "         33.4712, 33.4250, 33.4256, 33.4559, 33.4651, 33.4694, 33.4638, 33.4674,\n",
      "         33.4709, 33.4387, 33.4197, 33.4589, 33.4172, 33.4605, 33.4697, 33.4643,\n",
      "         33.4659, 33.4512, 33.4699, 33.4108, 33.4300, 33.4705, 33.4062, 33.4615,\n",
      "Iteration 13, Accuracy: 10.42689 tensor([[33.3881, 33.4531, 33.4712, 33.4430, 33.4497, 33.4675, 33.4713, 33.4701,\n",
      "         33.4321, 33.4702, 33.4701, 33.4706, 33.4559, 33.4567, 33.4627, 33.4439,\n",
      "         33.4697, 33.4077, 33.4236, 33.4671, 33.4673, 33.4700, 33.4647, 33.4712,\n",
      "         33.4692, 33.4449, 33.4684, 33.4698, 33.4691, 33.4397, 33.4658, 33.4705,\n",
      "         33.4498, 33.4362, 33.4665, 33.4421, 33.4687, 33.4498, 33.4605, 33.4692,\n",
      "         33.4676, 33.4708, 33.4286, 33.4601, 33.4607, 33.4649, 33.4714, 33.4638,\n",
      "         33.4467, 33.4628, 33.4651, 33.4680, 33.4669, 33.4364, 33.4257, 33.4475,\n",
      "         33.4700, 33.4504, 33.4633, 33.4538, 33.4664, 33.4646, 33.4686, 33.4660,\n",
      "         33.4700, 33.4449, 33.4711, 33.4700, 33.3994, 33.4448, 33.4692, 33.4527,\n",
      "         33.4693, 33.4304, 33.4503, 33.4694, 33.4711, 33.4690, 33.4567, 33.4694,\n",
      "         33.4710, 33.4493, 33.4216, 33.4701, 33.4350, 33.4626, 33.4696, 33.4657,\n",
      "         33.4667, 33.4474, 33.4696, 33.4185, 33.4217, 33.4680, 33.4400, 33.4614,\n",
      "Iteration 14, Accuracy: 12.14971 tensor([[33.3883, 33.4581, 33.4634, 33.4340, 33.4465, 33.4694, 33.4699, 33.4697,\n",
      "         33.4373, 33.4686, 33.4700, 33.4694, 33.4228, 33.4344, 33.4459, 33.4446,\n",
      "         33.4675, 33.4037, 33.4309, 33.4641, 33.4682, 33.4700, 33.4684, 33.4710,\n",
      "         33.4625, 33.4367, 33.4711, 33.4682, 33.4677, 33.4385, 33.4657, 33.4706,\n",
      "         33.4458, 33.4102, 33.4672, 33.4349, 33.4710, 33.4590, 33.4586, 33.4679,\n",
      "         33.4664, 33.4690, 33.4026, 33.4618, 33.4450, 33.4663, 33.4636, 33.4624,\n",
      "         33.4393, 33.4686, 33.4645, 33.4697, 33.4673, 33.4622, 33.4528, 33.4460,\n",
      "         33.4544, 33.4532, 33.4609, 33.4639, 33.4654, 33.4674, 33.4694, 33.4692,\n",
      "         33.4696, 33.4406, 33.4614, 33.4683, 33.3991, 33.4638, 33.4685, 33.4518,\n",
      "         33.4695, 33.4262, 33.4293, 33.4491, 33.4624, 33.4693, 33.4648, 33.4669,\n",
      "         33.4708, 33.4469, 33.4201, 33.4591, 33.4201, 33.4571, 33.4694, 33.4638,\n",
      "         33.4646, 33.4504, 33.4699, 33.4137, 33.4397, 33.4704, 33.4021, 33.4558,\n",
      "Iteration 15, Accuracy: 11.70948 tensor([[33.3883, 33.4597, 33.4660, 33.4366, 33.4445, 33.4694, 33.4705, 33.4698,\n",
      "         33.4358, 33.4688, 33.4700, 33.4695, 33.4245, 33.4317, 33.4481, 33.4439,\n",
      "         33.4680, 33.4073, 33.4307, 33.4637, 33.4683, 33.4701, 33.4686, 33.4709,\n",
      "         33.4627, 33.4358, 33.4711, 33.4684, 33.4678, 33.4384, 33.4668, 33.4706,\n",
      "         33.4475, 33.4077, 33.4672, 33.4373, 33.4709, 33.4590, 33.4610, 33.4681,\n",
      "         33.4663, 33.4701, 33.4016, 33.4591, 33.4420, 33.4663, 33.4653, 33.4629,\n",
      "         33.4408, 33.4692, 33.4648, 33.4697, 33.4677, 33.4641, 33.4548, 33.4489,\n",
      "         33.4542, 33.4474, 33.4611, 33.4665, 33.4656, 33.4674, 33.4696, 33.4692,\n",
      "         33.4698, 33.4439, 33.4633, 33.4687, 33.3994, 33.4633, 33.4685, 33.4527,\n",
      "         33.4701, 33.4205, 33.4301, 33.4499, 33.4621, 33.4693, 33.4650, 33.4670,\n",
      "         33.4709, 33.4387, 33.4229, 33.4568, 33.4152, 33.4601, 33.4692, 33.4643,\n",
      "         33.4647, 33.4523, 33.4700, 33.4119, 33.4418, 33.4707, 33.4027, 33.4581,\n",
      "Iteration 16, Accuracy: 11.25600 tensor([[33.3883, 33.4589, 33.4667, 33.4361, 33.4471, 33.4685, 33.4705, 33.4696,\n",
      "         33.4389, 33.4690, 33.4699, 33.4695, 33.4330, 33.4365, 33.4534, 33.4423,\n",
      "         33.4683, 33.4050, 33.4298, 33.4642, 33.4673, 33.4700, 33.4675, 33.4714,\n",
      "         33.4665, 33.4359, 33.4709, 33.4680, 33.4679, 33.4385, 33.4655, 33.4707,\n",
      "         33.4460, 33.4085, 33.4669, 33.4320, 33.4703, 33.4588, 33.4610, 33.4680,\n",
      "         33.4661, 33.4702, 33.4035, 33.4606, 33.4461, 33.4658, 33.4674, 33.4617,\n",
      "         33.4406, 33.4695, 33.4636, 33.4693, 33.4658, 33.4591, 33.4417, 33.4483,\n",
      "         33.4543, 33.4470, 33.4607, 33.4643, 33.4651, 33.4665, 33.4689, 33.4682,\n",
      "         33.4694, 33.4422, 33.4640, 33.4686, 33.4004, 33.4604, 33.4683, 33.4519,\n",
      "         33.4708, 33.4254, 33.4246, 33.4515, 33.4655, 33.4694, 33.4641, 33.4662,\n",
      "         33.4709, 33.4484, 33.4194, 33.4588, 33.4169, 33.4598, 33.4702, 33.4642,\n",
      "         33.4649, 33.4511, 33.4699, 33.4115, 33.4300, 33.4702, 33.4073, 33.4601,\n",
      "Iteration 17, Accuracy: 11.14999 tensor([[33.3883, 33.4616, 33.4683, 33.4379, 33.4450, 33.4678, 33.4710, 33.4695,\n",
      "         33.4351, 33.4693, 33.4702, 33.4696, 33.4277, 33.4367, 33.4484, 33.4462,\n",
      "         33.4680, 33.4046, 33.4308, 33.4643, 33.4673, 33.4699, 33.4673, 33.4714,\n",
      "         33.4653, 33.4383, 33.4709, 33.4681, 33.4675, 33.4391, 33.4654, 33.4704,\n",
      "         33.4472, 33.4075, 33.4668, 33.4354, 33.4706, 33.4586, 33.4604, 33.4679,\n",
      "         33.4663, 33.4706, 33.4003, 33.4609, 33.4469, 33.4652, 33.4676, 33.4619,\n",
      "         33.4396, 33.4671, 33.4645, 33.4693, 33.4657, 33.4603, 33.4448, 33.4486,\n",
      "         33.4590, 33.4439, 33.4598, 33.4632, 33.4654, 33.4664, 33.4687, 33.4678,\n",
      "         33.4694, 33.4487, 33.4640, 33.4689, 33.4009, 33.4625, 33.4682, 33.4531,\n",
      "         33.4706, 33.4234, 33.4335, 33.4553, 33.4659, 33.4693, 33.4645, 33.4662,\n",
      "         33.4705, 33.4495, 33.4194, 33.4602, 33.4195, 33.4607, 33.4706, 33.4647,\n",
      "         33.4641, 33.4513, 33.4688, 33.4108, 33.4389, 33.4701, 33.4010, 33.4578,\n",
      "Iteration 18, Accuracy: 11.44963 tensor([[33.3879, 33.4565, 33.4713, 33.4437, 33.4481, 33.4667, 33.4713, 33.4701,\n",
      "         33.4362, 33.4700, 33.4698, 33.4707, 33.4575, 33.4525, 33.4625, 33.4492,\n",
      "         33.4678, 33.4148, 33.4336, 33.4653, 33.4677, 33.4696, 33.4637, 33.4711,\n",
      "         33.4674, 33.4499, 33.4676, 33.4695, 33.4691, 33.4416, 33.4658, 33.4701,\n",
      "         33.4526, 33.4334, 33.4651, 33.4465, 33.4684, 33.4525, 33.4624, 33.4691,\n",
      "         33.4669, 33.4708, 33.4245, 33.4602, 33.4642, 33.4657, 33.4712, 33.4618,\n",
      "         33.4440, 33.4651, 33.4642, 33.4668, 33.4662, 33.4412, 33.4289, 33.4453,\n",
      "         33.4714, 33.4478, 33.4608, 33.4581, 33.4658, 33.4639, 33.4683, 33.4645,\n",
      "         33.4700, 33.4470, 33.4716, 33.4691, 33.4066, 33.4466, 33.4687, 33.4527,\n",
      "         33.4689, 33.4295, 33.4574, 33.4708, 33.4708, 33.4695, 33.4610, 33.4677,\n",
      "         33.4708, 33.4468, 33.4230, 33.4684, 33.4449, 33.4625, 33.4691, 33.4648,\n",
      "         33.4674, 33.4485, 33.4693, 33.4183, 33.4278, 33.4681, 33.4394, 33.4618,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, Accuracy: 11.08145 tensor([[33.3883, 33.4600, 33.4668, 33.4368, 33.4472, 33.4688, 33.4706, 33.4698,\n",
      "         33.4361, 33.4688, 33.4698, 33.4693, 33.4288, 33.4394, 33.4488, 33.4424,\n",
      "         33.4704, 33.4016, 33.4211, 33.4651, 33.4682, 33.4699, 33.4673, 33.4714,\n",
      "         33.4655, 33.4373, 33.4708, 33.4681, 33.4684, 33.4391, 33.4654, 33.4706,\n",
      "         33.4490, 33.4119, 33.4674, 33.4295, 33.4704, 33.4548, 33.4609, 33.4684,\n",
      "         33.4678, 33.4698, 33.4045, 33.4609, 33.4508, 33.4660, 33.4688, 33.4620,\n",
      "         33.4392, 33.4698, 33.4640, 33.4685, 33.4651, 33.4614, 33.4528, 33.4468,\n",
      "         33.4563, 33.4532, 33.4615, 33.4662, 33.4649, 33.4657, 33.4691, 33.4688,\n",
      "         33.4695, 33.4463, 33.4647, 33.4688, 33.3958, 33.4599, 33.4682, 33.4507,\n",
      "         33.4712, 33.4259, 33.4269, 33.4589, 33.4661, 33.4694, 33.4612, 33.4668,\n",
      "         33.4708, 33.4482, 33.4244, 33.4611, 33.4185, 33.4594, 33.4702, 33.4637,\n",
      "         33.4658, 33.4497, 33.4700, 33.4172, 33.4383, 33.4703, 33.4057, 33.4612,\n",
      "Iteration 20, Accuracy: 10.70015 tensor([[33.3881, 33.4570, 33.4710, 33.4401, 33.4467, 33.4686, 33.4713, 33.4703,\n",
      "         33.4333, 33.4700, 33.4700, 33.4703, 33.4498, 33.4471, 33.4594, 33.4471,\n",
      "         33.4704, 33.4115, 33.4288, 33.4674, 33.4681, 33.4697, 33.4658, 33.4715,\n",
      "         33.4695, 33.4412, 33.4693, 33.4686, 33.4693, 33.4401, 33.4658, 33.4706,\n",
      "         33.4500, 33.4260, 33.4680, 33.4351, 33.4686, 33.4546, 33.4588, 33.4692,\n",
      "         33.4673, 33.4708, 33.4252, 33.4615, 33.4601, 33.4673, 33.4712, 33.4616,\n",
      "         33.4427, 33.4682, 33.4630, 33.4672, 33.4653, 33.4470, 33.4371, 33.4459,\n",
      "         33.4691, 33.4516, 33.4617, 33.4612, 33.4646, 33.4649, 33.4684, 33.4677,\n",
      "         33.4697, 33.4469, 33.4707, 33.4698, 33.4003, 33.4542, 33.4677, 33.4514,\n",
      "         33.4701, 33.4267, 33.4458, 33.4690, 33.4711, 33.4695, 33.4591, 33.4676,\n",
      "         33.4708, 33.4490, 33.4250, 33.4665, 33.4262, 33.4608, 33.4698, 33.4643,\n",
      "         33.4684, 33.4464, 33.4697, 33.4156, 33.4230, 33.4697, 33.4307, 33.4621,\n",
      "Iteration 21, Accuracy: 11.16904 tensor([[33.3884, 33.4588, 33.4633, 33.4363, 33.4457, 33.4696, 33.4686, 33.4697,\n",
      "         33.4379, 33.4686, 33.4699, 33.4689, 33.4235, 33.4302, 33.4479, 33.4428,\n",
      "         33.4660, 33.4042, 33.4324, 33.4644, 33.4683, 33.4700, 33.4689, 33.4704,\n",
      "         33.4617, 33.4396, 33.4711, 33.4683, 33.4680, 33.4388, 33.4657, 33.4705,\n",
      "         33.4435, 33.4065, 33.4672, 33.4343, 33.4711, 33.4576, 33.4578, 33.4682,\n",
      "         33.4666, 33.4689, 33.3990, 33.4590, 33.4426, 33.4670, 33.4652, 33.4627,\n",
      "         33.4404, 33.4695, 33.4644, 33.4701, 33.4657, 33.4642, 33.4586, 33.4488,\n",
      "         33.4495, 33.4482, 33.4617, 33.4659, 33.4655, 33.4680, 33.4697, 33.4697,\n",
      "         33.4698, 33.4471, 33.4582, 33.4685, 33.4019, 33.4638, 33.4681, 33.4513,\n",
      "         33.4683, 33.4211, 33.4255, 33.4490, 33.4595, 33.4691, 33.4644, 33.4669,\n",
      "         33.4710, 33.4378, 33.4217, 33.4558, 33.4156, 33.4553, 33.4687, 33.4647,\n",
      "         33.4658, 33.4494, 33.4698, 33.4092, 33.4405, 33.4710, 33.4001, 33.4545,\n",
      "Iteration 22, Accuracy: 10.90925 tensor([[33.3883, 33.4586, 33.4672, 33.4380, 33.4449, 33.4691, 33.4709, 33.4699,\n",
      "         33.4374, 33.4689, 33.4701, 33.4697, 33.4321, 33.4360, 33.4478, 33.4429,\n",
      "         33.4680, 33.4063, 33.4267, 33.4651, 33.4676, 33.4700, 33.4680, 33.4713,\n",
      "         33.4663, 33.4339, 33.4710, 33.4684, 33.4684, 33.4378, 33.4657, 33.4707,\n",
      "         33.4482, 33.4113, 33.4670, 33.4259, 33.4706, 33.4547, 33.4611, 33.4684,\n",
      "         33.4665, 33.4702, 33.4079, 33.4610, 33.4483, 33.4663, 33.4681, 33.4623,\n",
      "         33.4399, 33.4697, 33.4643, 33.4693, 33.4656, 33.4565, 33.4459, 33.4476,\n",
      "         33.4557, 33.4444, 33.4606, 33.4652, 33.4653, 33.4668, 33.4692, 33.4688,\n",
      "         33.4697, 33.4478, 33.4627, 33.4690, 33.3999, 33.4632, 33.4682, 33.4526,\n",
      "         33.4712, 33.4250, 33.4267, 33.4513, 33.4652, 33.4693, 33.4638, 33.4674,\n",
      "         33.4710, 33.4379, 33.4201, 33.4623, 33.4114, 33.4600, 33.4697, 33.4644,\n",
      "         33.4660, 33.4513, 33.4698, 33.4133, 33.4304, 33.4705, 33.4093, 33.4606,\n",
      "Iteration 23, Accuracy: 10.54556 tensor([[33.3881, 33.4531, 33.4712, 33.4386, 33.4479, 33.4683, 33.4712, 33.4705,\n",
      "         33.4351, 33.4704, 33.4700, 33.4706, 33.4559, 33.4487, 33.4596, 33.4494,\n",
      "         33.4700, 33.4085, 33.4290, 33.4674, 33.4675, 33.4701, 33.4662, 33.4715,\n",
      "         33.4693, 33.4429, 33.4692, 33.4694, 33.4695, 33.4397, 33.4661, 33.4707,\n",
      "         33.4525, 33.4289, 33.4666, 33.4397, 33.4685, 33.4533, 33.4589, 33.4695,\n",
      "         33.4677, 33.4710, 33.4246, 33.4625, 33.4592, 33.4679, 33.4714, 33.4624,\n",
      "         33.4407, 33.4669, 33.4638, 33.4679, 33.4644, 33.4392, 33.4294, 33.4474,\n",
      "         33.4703, 33.4508, 33.4625, 33.4606, 33.4650, 33.4654, 33.4686, 33.4671,\n",
      "         33.4700, 33.4453, 33.4710, 33.4692, 33.4034, 33.4505, 33.4681, 33.4503,\n",
      "         33.4702, 33.4292, 33.4490, 33.4696, 33.4701, 33.4696, 33.4613, 33.4682,\n",
      "         33.4709, 33.4489, 33.4204, 33.4658, 33.4350, 33.4587, 33.4702, 33.4653,\n",
      "         33.4686, 33.4479, 33.4697, 33.4143, 33.4273, 33.4700, 33.4348, 33.4618,\n",
      "Iteration 24, Accuracy: 10.29181 tensor([[33.3881, 33.4585, 33.4713, 33.4394, 33.4463, 33.4678, 33.4712, 33.4706,\n",
      "         33.4384, 33.4704, 33.4693, 33.4701, 33.4507, 33.4426, 33.4587, 33.4508,\n",
      "         33.4700, 33.4148, 33.4300, 33.4661, 33.4671, 33.4701, 33.4654, 33.4715,\n",
      "         33.4696, 33.4467, 33.4682, 33.4689, 33.4697, 33.4396, 33.4658, 33.4709,\n",
      "         33.4542, 33.4215, 33.4664, 33.4407, 33.4676, 33.4507, 33.4600, 33.4693,\n",
      "         33.4672, 33.4712, 33.4209, 33.4597, 33.4601, 33.4683, 33.4712, 33.4613,\n",
      "         33.4418, 33.4689, 33.4619, 33.4678, 33.4642, 33.4385, 33.4283, 33.4481,\n",
      "         33.4709, 33.4517, 33.4617, 33.4630, 33.4635, 33.4655, 33.4681, 33.4670,\n",
      "         33.4699, 33.4473, 33.4714, 33.4690, 33.4040, 33.4542, 33.4669, 33.4499,\n",
      "         33.4698, 33.4251, 33.4530, 33.4696, 33.4711, 33.4699, 33.4614, 33.4674,\n",
      "         33.4704, 33.4460, 33.4208, 33.4642, 33.4367, 33.4574, 33.4690, 33.4640,\n",
      "         33.4690, 33.4484, 33.4690, 33.4125, 33.4303, 33.4702, 33.4299, 33.4615,\n",
      "Iteration 25, Accuracy: 9.97828 tensor([[33.3883, 33.4576, 33.4674, 33.4380, 33.4459, 33.4691, 33.4706, 33.4699,\n",
      "         33.4376, 33.4690, 33.4701, 33.4697, 33.4324, 33.4363, 33.4475, 33.4415,\n",
      "         33.4681, 33.4073, 33.4283, 33.4650, 33.4676, 33.4699, 33.4681, 33.4713,\n",
      "         33.4648, 33.4351, 33.4709, 33.4684, 33.4683, 33.4378, 33.4657, 33.4707,\n",
      "         33.4483, 33.4133, 33.4681, 33.4260, 33.4707, 33.4546, 33.4595, 33.4685,\n",
      "         33.4665, 33.4701, 33.4067, 33.4609, 33.4507, 33.4663, 33.4671, 33.4623,\n",
      "         33.4406, 33.4698, 33.4644, 33.4695, 33.4654, 33.4576, 33.4426, 33.4477,\n",
      "         33.4557, 33.4445, 33.4606, 33.4652, 33.4652, 33.4670, 33.4692, 33.4688,\n",
      "         33.4696, 33.4478, 33.4626, 33.4689, 33.4010, 33.4634, 33.4685, 33.4526,\n",
      "         33.4712, 33.4244, 33.4198, 33.4526, 33.4652, 33.4692, 33.4643, 33.4673,\n",
      "         33.4710, 33.4380, 33.4189, 33.4617, 33.4102, 33.4598, 33.4701, 33.4645,\n",
      "         33.4659, 33.4463, 33.4699, 33.4129, 33.4277, 33.4702, 33.4085, 33.4595,\n",
      "Iteration 26, Accuracy: 9.61544 tensor([[33.3881, 33.4560, 33.4712, 33.4409, 33.4492, 33.4679, 33.4714, 33.4702,\n",
      "         33.4343, 33.4701, 33.4701, 33.4707, 33.4566, 33.4571, 33.4619, 33.4488,\n",
      "         33.4689, 33.4105, 33.4256, 33.4672, 33.4667, 33.4696, 33.4648, 33.4714,\n",
      "         33.4696, 33.4464, 33.4687, 33.4698, 33.4692, 33.4399, 33.4637, 33.4704,\n",
      "         33.4516, 33.4330, 33.4663, 33.4390, 33.4687, 33.4510, 33.4604, 33.4691,\n",
      "         33.4682, 33.4705, 33.4233, 33.4601, 33.4625, 33.4656, 33.4714, 33.4632,\n",
      "         33.4462, 33.4632, 33.4651, 33.4677, 33.4667, 33.4335, 33.4281, 33.4468,\n",
      "         33.4702, 33.4418, 33.4629, 33.4543, 33.4663, 33.4647, 33.4686, 33.4659,\n",
      "         33.4700, 33.4499, 33.4711, 33.4699, 33.3980, 33.4434, 33.4691, 33.4522,\n",
      "         33.4697, 33.4304, 33.4517, 33.4697, 33.4713, 33.4695, 33.4584, 33.4687,\n",
      "         33.4710, 33.4458, 33.4198, 33.4690, 33.4315, 33.4630, 33.4702, 33.4656,\n",
      "         33.4673, 33.4510, 33.4695, 33.4170, 33.4239, 33.4684, 33.4370, 33.4621,\n",
      "Iteration 27, Accuracy: 9.61288 tensor([[33.3883, 33.4602, 33.4662, 33.4363, 33.4460, 33.4688, 33.4705, 33.4695,\n",
      "         33.4372, 33.4689, 33.4698, 33.4697, 33.4281, 33.4339, 33.4479, 33.4409,\n",
      "         33.4682, 33.4056, 33.4286, 33.4640, 33.4684, 33.4701, 33.4678, 33.4714,\n",
      "         33.4656, 33.4354, 33.4710, 33.4681, 33.4682, 33.4386, 33.4665, 33.4704,\n",
      "         33.4463, 33.4112, 33.4670, 33.4321, 33.4707, 33.4591, 33.4609, 33.4684,\n",
      "         33.4677, 33.4700, 33.4030, 33.4595, 33.4483, 33.4660, 33.4672, 33.4621,\n",
      "         33.4420, 33.4692, 33.4640, 33.4690, 33.4651, 33.4597, 33.4466, 33.4458,\n",
      "         33.4551, 33.4446, 33.4608, 33.4654, 33.4652, 33.4666, 33.4690, 33.4684,\n",
      "         33.4696, 33.4468, 33.4641, 33.4690, 33.3995, 33.4617, 33.4683, 33.4528,\n",
      "         33.4711, 33.4250, 33.4256, 33.4548, 33.4657, 33.4693, 33.4637, 33.4666,\n",
      "         33.4708, 33.4475, 33.4191, 33.4618, 33.4173, 33.4585, 33.4698, 33.4642,\n",
      "         33.4652, 33.4480, 33.4700, 33.4105, 33.4294, 33.4703, 33.4058, 33.4600,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, Accuracy: 9.57150 tensor([[33.3883, 33.4585, 33.4694, 33.4388, 33.4482, 33.4685, 33.4711, 33.4697,\n",
      "         33.4345, 33.4695, 33.4700, 33.4702, 33.4387, 33.4435, 33.4528, 33.4452,\n",
      "         33.4684, 33.4049, 33.4216, 33.4652, 33.4677, 33.4693, 33.4665, 33.4715,\n",
      "         33.4692, 33.4386, 33.4700, 33.4683, 33.4689, 33.4396, 33.4658, 33.4701,\n",
      "         33.4511, 33.4181, 33.4688, 33.4292, 33.4698, 33.4545, 33.4595, 33.4691,\n",
      "         33.4676, 33.4704, 33.4109, 33.4614, 33.4542, 33.4660, 33.4706, 33.4608,\n",
      "         33.4393, 33.4688, 33.4635, 33.4673, 33.4654, 33.4559, 33.4444, 33.4462,\n",
      "         33.4623, 33.4480, 33.4612, 33.4644, 33.4646, 33.4647, 33.4685, 33.4677,\n",
      "         33.4696, 33.4419, 33.4677, 33.4687, 33.3965, 33.4572, 33.4683, 33.4505,\n",
      "         33.4708, 33.4274, 33.4316, 33.4623, 33.4700, 33.4695, 33.4574, 33.4657,\n",
      "         33.4707, 33.4452, 33.4222, 33.4652, 33.4168, 33.4614, 33.4701, 33.4643,\n",
      "         33.4664, 33.4483, 33.4697, 33.4175, 33.4264, 33.4696, 33.4135, 33.4613,\n",
      "Iteration 29, Accuracy: 9.32605 tensor([[33.3883, 33.4595, 33.4645, 33.4360, 33.4465, 33.4694, 33.4703, 33.4699,\n",
      "         33.4370, 33.4689, 33.4702, 33.4690, 33.4269, 33.4371, 33.4482, 33.4409,\n",
      "         33.4679, 33.4030, 33.4316, 33.4650, 33.4684, 33.4699, 33.4687, 33.4713,\n",
      "         33.4647, 33.4309, 33.4711, 33.4684, 33.4682, 33.4384, 33.4658, 33.4708,\n",
      "         33.4478, 33.4093, 33.4671, 33.4315, 33.4708, 33.4564, 33.4594, 33.4682,\n",
      "         33.4685, 33.4701, 33.4023, 33.4603, 33.4423, 33.4661, 33.4665, 33.4623,\n",
      "         33.4400, 33.4699, 33.4644, 33.4697, 33.4650, 33.4608, 33.4510, 33.4487,\n",
      "         33.4523, 33.4481, 33.4613, 33.4646, 33.4655, 33.4676, 33.4694, 33.4692,\n",
      "         33.4697, 33.4428, 33.4603, 33.4693, 33.4016, 33.4620, 33.4684, 33.4526,\n",
      "         33.4703, 33.4261, 33.4233, 33.4525, 33.4635, 33.4693, 33.4650, 33.4668,\n",
      "         33.4710, 33.4471, 33.4214, 33.4598, 33.4136, 33.4581, 33.4698, 33.4649,\n",
      "         33.4654, 33.4521, 33.4692, 33.4122, 33.4310, 33.4706, 33.4038, 33.4582,\n",
      "Iteration 30, Accuracy: 9.63327 tensor([[33.3879, 33.4575, 33.4697, 33.4483, 33.4523, 33.4655, 33.4691, 33.4690,\n",
      "         33.4613, 33.4548, 33.4637, 33.4701, 33.4686, 33.4691, 33.4713, 33.4523,\n",
      "         33.4665, 33.4258, 33.4289, 33.4676, 33.4635, 33.4675, 33.4629, 33.4667,\n",
      "         33.4659, 33.4534, 33.4653, 33.4684, 33.4690, 33.4654, 33.4630, 33.4687,\n",
      "         33.4533, 33.4604, 33.4627, 33.4506, 33.4676, 33.4230, 33.4600, 33.4686,\n",
      "         33.4669, 33.4680, 33.4345, 33.4600, 33.4706, 33.4647, 33.4698, 33.4675,\n",
      "         33.4658, 33.4548, 33.4677, 33.4666, 33.4671, 33.4272, 33.4285, 33.4537,\n",
      "         33.4715, 33.4374, 33.4667, 33.4483, 33.4696, 33.4650, 33.4687, 33.4628,\n",
      "         33.4707, 33.4494, 33.4709, 33.4695, 33.4113, 33.4084, 33.4700, 33.4527,\n",
      "         33.4654, 33.4387, 33.4679, 33.4715, 33.4702, 33.4698, 33.4564, 33.4703,\n",
      "         33.4645, 33.4475, 33.4228, 33.4709, 33.4600, 33.4687, 33.4686, 33.4678,\n",
      "         33.4682, 33.4540, 33.4648, 33.4150, 33.4283, 33.4631, 33.4585, 33.4658,\n",
      "Iteration 31, Accuracy: 9.43691 tensor([[33.3883, 33.4581, 33.4679, 33.4373, 33.4446, 33.4686, 33.4704, 33.4698,\n",
      "         33.4345, 33.4688, 33.4699, 33.4693, 33.4284, 33.4314, 33.4514, 33.4443,\n",
      "         33.4683, 33.4063, 33.4323, 33.4644, 33.4672, 33.4700, 33.4675, 33.4711,\n",
      "         33.4664, 33.4402, 33.4708, 33.4680, 33.4677, 33.4396, 33.4652, 33.4707,\n",
      "         33.4502, 33.4080, 33.4670, 33.4352, 33.4707, 33.4588, 33.4609, 33.4682,\n",
      "         33.4678, 33.4701, 33.3999, 33.4595, 33.4422, 33.4662, 33.4665, 33.4617,\n",
      "         33.4398, 33.4693, 33.4638, 33.4691, 33.4651, 33.4609, 33.4485, 33.4483,\n",
      "         33.4612, 33.4479, 33.4606, 33.4648, 33.4653, 33.4665, 33.4691, 33.4685,\n",
      "         33.4695, 33.4452, 33.4652, 33.4681, 33.4029, 33.4633, 33.4682, 33.4517,\n",
      "         33.4702, 33.4217, 33.4339, 33.4520, 33.4666, 33.4693, 33.4638, 33.4665,\n",
      "         33.4707, 33.4445, 33.4187, 33.4571, 33.4191, 33.4589, 33.4699, 33.4635,\n",
      "         33.4655, 33.4500, 33.4698, 33.4115, 33.4376, 33.4705, 33.4022, 33.4599,\n",
      "Iteration 32, Accuracy: 9.72149 tensor([[33.3880, 33.4586, 33.4705, 33.4463, 33.4508, 33.4661, 33.4702, 33.4696,\n",
      "         33.4508, 33.4631, 33.4697, 33.4710, 33.4647, 33.4636, 33.4710, 33.4525,\n",
      "         33.4680, 33.4198, 33.4261, 33.4672, 33.4636, 33.4678, 33.4622, 33.4695,\n",
      "         33.4679, 33.4500, 33.4658, 33.4706, 33.4688, 33.4572, 33.4626, 33.4692,\n",
      "         33.4551, 33.4474, 33.4639, 33.4484, 33.4665, 33.4436, 33.4612, 33.4689,\n",
      "         33.4689, 33.4696, 33.4268, 33.4614, 33.4693, 33.4656, 33.4702, 33.4647,\n",
      "         33.4593, 33.4577, 33.4665, 33.4663, 33.4674, 33.4257, 33.4262, 33.4501,\n",
      "         33.4716, 33.4437, 33.4651, 33.4449, 33.4676, 33.4640, 33.4684, 33.4625,\n",
      "         33.4708, 33.4487, 33.4713, 33.4695, 33.4064, 33.4180, 33.4698, 33.4523,\n",
      "         33.4670, 33.4356, 33.4653, 33.4714, 33.4704, 33.4700, 33.4521, 33.4691,\n",
      "         33.4700, 33.4454, 33.4204, 33.4711, 33.4535, 33.4682, 33.4685, 33.4672,\n",
      "         33.4684, 33.4499, 33.4672, 33.4076, 33.4276, 33.4657, 33.4472, 33.4650,\n",
      "Iteration 33, Accuracy: 10.50400 tensor([[33.3881, 33.4578, 33.4711, 33.4395, 33.4460, 33.4685, 33.4712, 33.4706,\n",
      "         33.4382, 33.4704, 33.4697, 33.4690, 33.4527, 33.4440, 33.4585, 33.4495,\n",
      "         33.4679, 33.4095, 33.4317, 33.4670, 33.4671, 33.4698, 33.4663, 33.4715,\n",
      "         33.4690, 33.4456, 33.4688, 33.4689, 33.4696, 33.4394, 33.4659, 33.4709,\n",
      "         33.4536, 33.4230, 33.4668, 33.4379, 33.4680, 33.4533, 33.4595, 33.4695,\n",
      "         33.4676, 33.4709, 33.4223, 33.4601, 33.4599, 33.4686, 33.4712, 33.4622,\n",
      "         33.4413, 33.4695, 33.4627, 33.4676, 33.4639, 33.4450, 33.4297, 33.4478,\n",
      "         33.4702, 33.4510, 33.4623, 33.4622, 33.4641, 33.4654, 33.4683, 33.4679,\n",
      "         33.4699, 33.4458, 33.4712, 33.4699, 33.4024, 33.4522, 33.4671, 33.4497,\n",
      "         33.4703, 33.4255, 33.4509, 33.4688, 33.4713, 33.4696, 33.4601, 33.4676,\n",
      "         33.4707, 33.4445, 33.4223, 33.4656, 33.4374, 33.4574, 33.4694, 33.4643,\n",
      "         33.4692, 33.4525, 33.4693, 33.4143, 33.4255, 33.4704, 33.4311, 33.4619,\n",
      "Iteration 34, Accuracy: 10.35556 tensor([[33.3879, 33.4599, 33.4704, 33.4474, 33.4516, 33.4658, 33.4698, 33.4696,\n",
      "         33.4536, 33.4604, 33.4688, 33.4707, 33.4645, 33.4653, 33.4713, 33.4513,\n",
      "         33.4663, 33.4240, 33.4265, 33.4680, 33.4642, 33.4675, 33.4624, 33.4683,\n",
      "         33.4672, 33.4505, 33.4658, 33.4701, 33.4691, 33.4628, 33.4648, 33.4690,\n",
      "         33.4532, 33.4505, 33.4637, 33.4484, 33.4669, 33.4432, 33.4613, 33.4690,\n",
      "         33.4674, 33.4689, 33.4245, 33.4583, 33.4697, 33.4643, 33.4702, 33.4659,\n",
      "         33.4626, 33.4534, 33.4672, 33.4667, 33.4672, 33.4239, 33.4279, 33.4534,\n",
      "         33.4716, 33.4421, 33.4653, 33.4488, 33.4684, 33.4641, 33.4687, 33.4624,\n",
      "         33.4708, 33.4487, 33.4713, 33.4696, 33.4108, 33.4150, 33.4701, 33.4533,\n",
      "         33.4666, 33.4356, 33.4645, 33.4712, 33.4707, 33.4696, 33.4560, 33.4700,\n",
      "         33.4688, 33.4456, 33.4211, 33.4715, 33.4539, 33.4678, 33.4686, 33.4674,\n",
      "         33.4678, 33.4529, 33.4653, 33.4095, 33.4239, 33.4640, 33.4510, 33.4665,\n",
      "Iteration 35, Accuracy: 10.81814 tensor([[33.3883, 33.4581, 33.4654, 33.4356, 33.4464, 33.4693, 33.4699, 33.4697,\n",
      "         33.4374, 33.4688, 33.4698, 33.4691, 33.4272, 33.4393, 33.4483, 33.4433,\n",
      "         33.4679, 33.4069, 33.4294, 33.4637, 33.4684, 33.4702, 33.4683, 33.4711,\n",
      "         33.4648, 33.4383, 33.4710, 33.4682, 33.4680, 33.4382, 33.4657, 33.4707,\n",
      "         33.4473, 33.4088, 33.4671, 33.4314, 33.4708, 33.4590, 33.4606, 33.4683,\n",
      "         33.4665, 33.4700, 33.4030, 33.4607, 33.4494, 33.4661, 33.4656, 33.4630,\n",
      "         33.4390, 33.4699, 33.4647, 33.4698, 33.4650, 33.4635, 33.4542, 33.4475,\n",
      "         33.4568, 33.4445, 33.4612, 33.4651, 33.4654, 33.4671, 33.4694, 33.4691,\n",
      "         33.4696, 33.4429, 33.4627, 33.4688, 33.4016, 33.4629, 33.4687, 33.4516,\n",
      "         33.4702, 33.4239, 33.4277, 33.4504, 33.4618, 33.4694, 33.4648, 33.4675,\n",
      "         33.4708, 33.4472, 33.4225, 33.4603, 33.4208, 33.4586, 33.4693, 33.4644,\n",
      "         33.4651, 33.4507, 33.4692, 33.4130, 33.4410, 33.4707, 33.4045, 33.4554,\n",
      "Iteration 36, Accuracy: 11.00499 tensor([[33.3881, 33.4584, 33.4712, 33.4382, 33.4476, 33.4685, 33.4713, 33.4704,\n",
      "         33.4370, 33.4702, 33.4698, 33.4697, 33.4504, 33.4465, 33.4593, 33.4451,\n",
      "         33.4684, 33.4076, 33.4271, 33.4668, 33.4671, 33.4695, 33.4660, 33.4715,\n",
      "         33.4696, 33.4443, 33.4687, 33.4688, 33.4695, 33.4398, 33.4660, 33.4707,\n",
      "         33.4540, 33.4257, 33.4671, 33.4403, 33.4683, 33.4537, 33.4603, 33.4693,\n",
      "         33.4674, 33.4709, 33.4234, 33.4602, 33.4601, 33.4681, 33.4712, 33.4621,\n",
      "         33.4409, 33.4691, 33.4631, 33.4674, 33.4640, 33.4460, 33.4292, 33.4493,\n",
      "         33.4699, 33.4456, 33.4623, 33.4614, 33.4642, 33.4653, 33.4684, 33.4678,\n",
      "         33.4699, 33.4432, 33.4710, 33.4696, 33.4022, 33.4496, 33.4672, 33.4499,\n",
      "         33.4702, 33.4285, 33.4487, 33.4690, 33.4712, 33.4696, 33.4602, 33.4673,\n",
      "         33.4707, 33.4456, 33.4183, 33.4653, 33.4284, 33.4585, 33.4695, 33.4642,\n",
      "         33.4688, 33.4501, 33.4687, 33.4157, 33.4276, 33.4700, 33.4315, 33.4628,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, Accuracy: 10.96311 tensor([[33.3881, 33.4531, 33.4712, 33.4433, 33.4497, 33.4667, 33.4710, 33.4701,\n",
      "         33.4371, 33.4700, 33.4700, 33.4708, 33.4556, 33.4563, 33.4631, 33.4449,\n",
      "         33.4694, 33.4122, 33.4206, 33.4665, 33.4674, 33.4691, 33.4637, 33.4709,\n",
      "         33.4692, 33.4415, 33.4679, 33.4698, 33.4689, 33.4401, 33.4659, 33.4704,\n",
      "         33.4523, 33.4351, 33.4660, 33.4443, 33.4682, 33.4515, 33.4600, 33.4688,\n",
      "         33.4677, 33.4702, 33.4233, 33.4617, 33.4625, 33.4646, 33.4713, 33.4624,\n",
      "         33.4438, 33.4636, 33.4647, 33.4670, 33.4666, 33.4395, 33.4260, 33.4490,\n",
      "         33.4705, 33.4476, 33.4618, 33.4543, 33.4661, 33.4635, 33.4685, 33.4652,\n",
      "         33.4701, 33.4465, 33.4714, 33.4696, 33.4001, 33.4437, 33.4692, 33.4524,\n",
      "         33.4691, 33.4293, 33.4520, 33.4696, 33.4711, 33.4694, 33.4575, 33.4681,\n",
      "         33.4709, 33.4498, 33.4237, 33.4691, 33.4364, 33.4634, 33.4699, 33.4653,\n",
      "         33.4667, 33.4489, 33.4702, 33.4159, 33.4232, 33.4675, 33.4384, 33.4627,\n",
      "Iteration 38, Accuracy: 11.00472 tensor([[33.3881, 33.4536, 33.4711, 33.4433, 33.4495, 33.4671, 33.4712, 33.4699,\n",
      "         33.4363, 33.4701, 33.4700, 33.4706, 33.4583, 33.4564, 33.4622, 33.4472,\n",
      "         33.4694, 33.4090, 33.4243, 33.4671, 33.4672, 33.4694, 33.4639, 33.4711,\n",
      "         33.4695, 33.4420, 33.4679, 33.4695, 33.4690, 33.4397, 33.4656, 33.4702,\n",
      "         33.4514, 33.4381, 33.4662, 33.4438, 33.4684, 33.4531, 33.4586, 33.4688,\n",
      "         33.4672, 33.4702, 33.4272, 33.4602, 33.4606, 33.4647, 33.4714, 33.4629,\n",
      "         33.4455, 33.4643, 33.4646, 33.4670, 33.4667, 33.4359, 33.4260, 33.4451,\n",
      "         33.4704, 33.4506, 33.4625, 33.4542, 33.4662, 33.4638, 33.4684, 33.4655,\n",
      "         33.4700, 33.4460, 33.4713, 33.4703, 33.3971, 33.4445, 33.4692, 33.4526,\n",
      "         33.4692, 33.4290, 33.4514, 33.4695, 33.4711, 33.4689, 33.4571, 33.4681,\n",
      "         33.4710, 33.4497, 33.4238, 33.4691, 33.4355, 33.4634, 33.4695, 33.4654,\n",
      "         33.4668, 33.4498, 33.4696, 33.4155, 33.4221, 33.4675, 33.4384, 33.4628,\n",
      "Iteration 39, Accuracy: 10.89031 tensor([[33.3879, 33.4577, 33.4698, 33.4482, 33.4523, 33.4654, 33.4691, 33.4692,\n",
      "         33.4618, 33.4539, 33.4628, 33.4701, 33.4676, 33.4691, 33.4713, 33.4522,\n",
      "         33.4665, 33.4265, 33.4327, 33.4663, 33.4625, 33.4674, 33.4629, 33.4665,\n",
      "         33.4653, 33.4531, 33.4652, 33.4680, 33.4690, 33.4649, 33.4623, 33.4686,\n",
      "         33.4533, 33.4609, 33.4625, 33.4507, 33.4676, 33.4243, 33.4600, 33.4684,\n",
      "         33.4669, 33.4682, 33.4315, 33.4599, 33.4707, 33.4648, 33.4698, 33.4678,\n",
      "         33.4648, 33.4535, 33.4678, 33.4667, 33.4682, 33.4265, 33.4296, 33.4537,\n",
      "         33.4715, 33.4376, 33.4668, 33.4508, 33.4698, 33.4649, 33.4688, 33.4627,\n",
      "         33.4707, 33.4495, 33.4710, 33.4685, 33.4145, 33.4056, 33.4698, 33.4527,\n",
      "         33.4652, 33.4405, 33.4665, 33.4714, 33.4701, 33.4697, 33.4581, 33.4703,\n",
      "         33.4642, 33.4475, 33.4230, 33.4708, 33.4609, 33.4687, 33.4686, 33.4677,\n",
      "         33.4682, 33.4539, 33.4649, 33.4103, 33.4292, 33.4628, 33.4561, 33.4656,\n",
      "Iteration 40, Accuracy: 10.87945 tensor([[33.3881, 33.4568, 33.4712, 33.4439, 33.4483, 33.4668, 33.4710, 33.4697,\n",
      "         33.4365, 33.4701, 33.4701, 33.4708, 33.4557, 33.4581, 33.4650, 33.4490,\n",
      "         33.4679, 33.4114, 33.4247, 33.4666, 33.4674, 33.4692, 33.4638, 33.4710,\n",
      "         33.4691, 33.4426, 33.4677, 33.4697, 33.4691, 33.4414, 33.4660, 33.4702,\n",
      "         33.4516, 33.4347, 33.4668, 33.4446, 33.4684, 33.4513, 33.4589, 33.4691,\n",
      "         33.4678, 33.4702, 33.4237, 33.4602, 33.4624, 33.4644, 33.4713, 33.4624,\n",
      "         33.4477, 33.4630, 33.4648, 33.4674, 33.4686, 33.4405, 33.4267, 33.4464,\n",
      "         33.4706, 33.4414, 33.4618, 33.4547, 33.4663, 33.4633, 33.4684, 33.4647,\n",
      "         33.4702, 33.4443, 33.4715, 33.4698, 33.3995, 33.4437, 33.4693, 33.4524,\n",
      "         33.4688, 33.4307, 33.4525, 33.4699, 33.4712, 33.4694, 33.4571, 33.4683,\n",
      "         33.4710, 33.4461, 33.4206, 33.4694, 33.4373, 33.4635, 33.4700, 33.4653,\n",
      "         33.4662, 33.4509, 33.4693, 33.4184, 33.4239, 33.4673, 33.4346, 33.4629,\n",
      "Iteration 41, Accuracy: 10.77372 tensor([[33.3881, 33.4571, 33.4710, 33.4394, 33.4463, 33.4688, 33.4712, 33.4704,\n",
      "         33.4386, 33.4701, 33.4695, 33.4694, 33.4504, 33.4450, 33.4561, 33.4451,\n",
      "         33.4683, 33.4081, 33.4286, 33.4667, 33.4681, 33.4697, 33.4662, 33.4714,\n",
      "         33.4691, 33.4420, 33.4690, 33.4681, 33.4695, 33.4404, 33.4659, 33.4708,\n",
      "         33.4522, 33.4231, 33.4668, 33.4366, 33.4683, 33.4529, 33.4617, 33.4694,\n",
      "         33.4679, 33.4710, 33.4215, 33.4609, 33.4568, 33.4682, 33.4711, 33.4615,\n",
      "         33.4410, 33.4695, 33.4623, 33.4676, 33.4637, 33.4484, 33.4353, 33.4506,\n",
      "         33.4678, 33.4511, 33.4615, 33.4650, 33.4639, 33.4655, 33.4685, 33.4680,\n",
      "         33.4697, 33.4426, 33.4699, 33.4692, 33.4043, 33.4571, 33.4675, 33.4501,\n",
      "         33.4704, 33.4276, 33.4426, 33.4681, 33.4702, 33.4696, 33.4598, 33.4673,\n",
      "         33.4706, 33.4448, 33.4230, 33.4642, 33.4284, 33.4574, 33.4706, 33.4640,\n",
      "         33.4688, 33.4506, 33.4693, 33.4145, 33.4257, 33.4703, 33.4296, 33.4621,\n",
      "Iteration 42, Accuracy: 11.29211 tensor([[33.3883, 33.4591, 33.4671, 33.4380, 33.4452, 33.4685, 33.4702, 33.4695,\n",
      "         33.4369, 33.4690, 33.4697, 33.4691, 33.4322, 33.4381, 33.4476, 33.4403,\n",
      "         33.4682, 33.4063, 33.4298, 33.4652, 33.4684, 33.4699, 33.4675, 33.4713,\n",
      "         33.4673, 33.4352, 33.4708, 33.4680, 33.4681, 33.4390, 33.4665, 33.4706,\n",
      "         33.4493, 33.4109, 33.4669, 33.4319, 33.4706, 33.4572, 33.4610, 33.4682,\n",
      "         33.4676, 33.4704, 33.4054, 33.4596, 33.4483, 33.4661, 33.4664, 33.4621,\n",
      "         33.4396, 33.4693, 33.4638, 33.4690, 33.4658, 33.4606, 33.4457, 33.4445,\n",
      "         33.4568, 33.4447, 33.4608, 33.4653, 33.4649, 33.4665, 33.4690, 33.4683,\n",
      "         33.4695, 33.4436, 33.4649, 33.4682, 33.4005, 33.4617, 33.4681, 33.4528,\n",
      "         33.4711, 33.4237, 33.4278, 33.4551, 33.4651, 33.4694, 33.4643, 33.4666,\n",
      "         33.4707, 33.4478, 33.4238, 33.4607, 33.4126, 33.4587, 33.4702, 33.4641,\n",
      "         33.4652, 33.4517, 33.4700, 33.4101, 33.4309, 33.4702, 33.4076, 33.4591,\n",
      "         33.4613, 33.3989, 33.4514, 33.4562, 33.4673]])"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1676337/2199582302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1676337/1482031453.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1676337/1482031453.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1676337/1482031453.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_samples, n_patches + 1, hidden_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n_samples, n_patches + 1, hidden_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (n_samples, n_patches + 1, hidden_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyvhr/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0.0\n",
    "total = 0.0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(trainloader, 1):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        outputs = model(inputs)\n",
    "        err = torch.abs(torch.mean(outputs,1)- targets).item()\n",
    "        total = i\n",
    "        accuracy += err\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(f\"Iteration {i}, Accuracy: {accuracy/total:1.5f} {outputs}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083a6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 15\n",
    "EMBED_DIM = PATCH_SIZE * PATCH_SIZE * 3\n",
    "NUM_PATCHES = 100\n",
    "IMG_SIZE = PATCH_SIZE * NUM_PATCHES\n",
    "HEADS = 5\n",
    "BLOCKS = 12\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=EMBED_DIM):\n",
    "    super().__init__()\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_patches = (img_size // patch_size) ** 2\n",
    "    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.movedim(x,3,1)\n",
    "    x = self.proj(x)       # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "    x = x.flatten(2)        # (n_samples, embed_dim, n_patches)\n",
    "    x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "\n",
    "  def __init__(self, dim, n_heads=HEADS, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "    super().__init__()\n",
    "    self.n_heads = n_heads\n",
    "    self.dim = dim\n",
    "    self.head_dim = dim // n_heads\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(dim, dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "    if dim != self.dim:\n",
    "      raise ValueError\n",
    "\n",
    "    qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
    "    qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)  # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
    "    qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "\n",
    "    # compute att matrices\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "    k_t = k.transpose(-2, -1)   # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "    dp = (q @ k_t) * self.scale\n",
    "\n",
    "    attn = dp.softmax(dim=-1)   # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "    attn = self.attn_drop(attn)\n",
    "\n",
    "    # compute weigthed avg\n",
    "    weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
    "    weighted_avg = weighted_avg.transpose(1, 2)  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "    weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "    # linear projection\n",
    "    x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
    "    x = self.proj_drop(x)        # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "    return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "      self.act = nn.GELU()\n",
    "      self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "      self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)   # (n_samples, n_patches + 1, hidden_features)\n",
    "      x = self.act(x)   # (n_samples, n_patches + 1, hidden_features)\n",
    "      x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "      x = self.fc2(x)   # (n_samples, n_patches + 1, hidden_features)\n",
    "      x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "\n",
    "      return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "      super().__init__()\n",
    "      self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "      self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p)\n",
    "      self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "      hidden_features = int(dim * mlp_ratio)\n",
    "      self.mlp = MLP(in_features=dim, hidden_features=hidden_features, out_features=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "      \n",
    "      x = x + self.attn(self.norm1(x))\n",
    "      x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "      return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  \n",
    "  def __init__(\n",
    "          self,\n",
    "          img_size=IMG_SIZE,\n",
    "          patch_size=PATCH_SIZE,\n",
    "          in_chans=3,\n",
    "          n_classes=1000,\n",
    "          embed_dim=EMBED_DIM,\n",
    "          depth=BLOCKS,\n",
    "          n_heads=HEADS,\n",
    "          mlp_ratio=4.,\n",
    "          qkv_bias=True,\n",
    "          p=0.,\n",
    "          attn_p=0.,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "    )\n",
    "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "    self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "    )\n",
    "    self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "    self.blocks = nn.ModuleList(\n",
    "        [\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                n_heads=n_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                p=p,\n",
    "                attn_p=attn_p,\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "    self.head = nn.Linear(embed_dim, n_classes)\n",
    "    self.lastConv = nn.Conv1d(embed_dim, 1,1,stride=1, padding=0)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    n_samples = x.shape[0]\n",
    "    x = self.patch_embed(x)  #(n_samples, n_patches, embed_dim)\n",
    "    \n",
    "    \n",
    "    cls_token = self.cls_token.expand(n_samples, -1, -1)  # (n_samples, 1, embed_dim)\n",
    "    x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
    "    #x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
    "    x = self.pos_drop(x)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "\n",
    "    x = self.norm(x)\n",
    "\n",
    "    cls_token_final = x[:, 0]  # just the CLS token\n",
    "    #x = self.head(cls_token_final)\n",
    "    #x = x.type(torch.DoubleTensor)\n",
    "    #x = torch.mean(x,1)\n",
    "    #x = torch.mean(x,1)\n",
    "    x = x.permute(0, 2, 1) \n",
    "    x = self.lastConv(x)\n",
    "    x = x.squeeze(1)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744425cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
